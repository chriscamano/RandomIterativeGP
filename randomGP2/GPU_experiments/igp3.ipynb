{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import time \n",
    "import gpytorch\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel, RBFKernel\n",
    "from gpytorch.priors import GammaPrior\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "# Set device and global dtype\n",
    "device = \"cuda:0\"\n",
    "global_dtype = torch.float32\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Add project source path\n",
    "notebook_dir = os.getcwd()\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '../code'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Import custom modules\n",
    "from gps import CholeskyGaussianProcess, IterativeGaussianProcess\n",
    "from util import train, eval, plot_gpr_results, fetch_uci_dataset, memory_dump\n",
    "from plotting import plot_gp_simple, plot_gp_sample, plot_gp_simple_regions\n",
    "\n",
    "# Enable autoreloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE (16599, 19)\n",
      "Dataset loaded\n",
      "torch.Size([1659, 17])\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = fetch_uci_dataset('bike',r\"C:\\Users\\fredw\\chris\\Research\\softki\\data\\uci_datasets\\uci_datasets\\elevators\\data.csv\",train_frac=1/10,val_frac=0)\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1659, 15])\n",
      "=== PERFORMANCE METRICS (Baseline: Cholesky GP) ===\n",
      "------------------------------------------------------------\n",
      "Cholesky GP Fit Time      : 0.0305 sec  (Baseline)\n",
      "Iterative GP Fit Time     : 0.0723 sec\n",
      "Fit Time Difference       : -0.0418 sec  (\u001b[91m-136.72%\u001b[0m)\n",
      "Speedup Factor            : 0.42×\n",
      "------------------------------------------------------------\n",
      "Cholesky GP Prediction Time  : 0.0838 sec  (Baseline)\n",
      "Iterative GP Prediction Time : 0.0020 sec\n",
      "Prediction Time Diff         : 0.0818 sec  (\u001b[92m+97.61%\u001b[0m)\n",
      "Speedup Factor               : 41.87×\n",
      "------------------------------------------------------------\n",
      "Cholesky GP Total Time   : 0.1143 sec  (Baseline)\n",
      "Iterative GP Total Time  : 0.0743 sec\n",
      "Total Time Difference    : 0.0400 sec  (\u001b[92m+35.00%\u001b[0m)\n",
      "Speedup Factor           : 1.54×\n",
      "------------------------------------------------------------\n",
      "=== ACCURACY METRICS ===\n",
      "Cholesky GP RMSE         : 0.4326  (Baseline)\n",
      "Iterative GP RMSE        : 0.4326\n",
      "RMSE Difference          : 0.0000\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming the following GP classes and kernels are defined elsewhere:\n",
    "# MaternKernel, GammaPrior, ScaleKernel, CholeskyGaussianProcess, IterativeGaussianProcess\n",
    "\n",
    "def run_cholesky_gp(train_x, train_y, test_x, test_y, device, global_dtype):\n",
    "    # Define the base kernel and the scaled kernel for the Cholesky-based GP\n",
    "    base_kernel = MaternKernel(ard_num_dims=train_x.shape[-1],\n",
    "                               lengthscale_prior=GammaPrior(3.0, 6.0),\n",
    "                               nu=1.5,        compute_covariance=False\n",
    ")\n",
    "    kernel = ScaleKernel(base_kernel, outputscale_prior=GammaPrior(2.0, 0.15)).to(device)\n",
    "    gpr = CholeskyGaussianProcess(kernel=kernel, dtype=global_dtype, noise=0.4, device=device)\n",
    "    \n",
    "    # Fit the GP and measure runtime\n",
    "    start_fit = time.time()\n",
    "    gpr.fit(train_x, train_y)\n",
    "    chol_fit_time = time.time() - start_fit\n",
    "    \n",
    "    # Prediction and runtime measurement\n",
    "    start_pred = time.time()\n",
    "    chol_mean, GP_covariance = gpr.predict(test_x)\n",
    "    chol_pred_time = time.time() - start_pred\n",
    "    \n",
    "    # Calculate RMSE for predictions\n",
    "    chol_rmse = torch.mean(torch.abs(chol_mean.detach().cpu() - test_y)).item()\n",
    "    chol_total_time = chol_fit_time + chol_pred_time\n",
    "\n",
    "\n",
    "    return (chol_total_time, chol_fit_time, chol_pred_time, chol_rmse)\n",
    "\n",
    "def run_iterative_gp(train_x, train_y, test_x, test_y, device, global_dtype):\n",
    "    # Define the base kernel and the scaled kernel for the Iterative GP\n",
    "    base_kernel = MaternKernel(ard_num_dims=train_x.shape[-1],\n",
    "                               lengthscale_prior=GammaPrior(3.0, 6.0),\n",
    "                               nu=1.5)\n",
    "    kernel = ScaleKernel(base_kernel, outputscale_prior=GammaPrior(2.0, 0.15)).to(device)\n",
    "    \n",
    "    igp = IterativeGaussianProcess(\n",
    "        kernel=kernel,\n",
    "        noise=0.4,\n",
    "        dtype=global_dtype,\n",
    "        device=device,\n",
    "        cg_tol=1e-3,\n",
    "        cg_max_iter=20,\n",
    "        warm_start=False,\n",
    "        num_probes=16,\n",
    "        precon_type=\"piv_chol\",\n",
    "        trace_backend=\"Hutch\",\n",
    "        verbose=False,\n",
    "        track_iterations=True,\n",
    "        pred_lanczos_rank=train_x.shape[0],\n",
    "        compute_covariance=False\n",
    "    )\n",
    "    \n",
    "    # Measure fitting time for the Iterative GP\n",
    "    start_fit = time.time()\n",
    "    igp.fit(train_x, train_y)\n",
    "    iter_fit_time = time.time() - start_fit\n",
    "    \n",
    "    # Measure prediction time\n",
    "    start_pred = time.time()\n",
    "    mean, covar = igp.predict(test_x)\n",
    "    iter_pred_time = time.time() - start_pred\n",
    "    \n",
    "    iter_total_time = iter_fit_time + iter_pred_time\n",
    "    \n",
    "    # Calculate RMSE for predictions\n",
    "    iter_rmse = torch.mean(torch.abs(mean.detach().cpu() - test_y)).item()\n",
    "    \n",
    "\n",
    "    # Cleanup\n",
    "    del igp, kernel, base_kernel\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return (iter_total_time, iter_fit_time, iter_pred_time, iter_rmse)\n",
    "\n",
    "# Run the Cholesky-based GP\n",
    "chol_total_time, chol_fit_time, chol_pred_time, chol_rmse = run_cholesky_gp(train_x, train_y, test_x, test_y, device, global_dtype)\n",
    "iter_total_time, iter_fit_time, iter_pred_time, iter_rmse = run_iterative_gp(train_x, train_y, test_x, test_y, device, global_dtype)\n",
    "\n",
    "fit_speedup = ((chol_fit_time - iter_fit_time) / chol_fit_time) * 100\n",
    "pred_speedup = ((chol_pred_time - iter_pred_time) / chol_pred_time) * 100\n",
    "total_speedup = ((chol_total_time - iter_total_time) / chol_total_time) * 100\n",
    "\n",
    "# Compute multiplicative speedup factors (e.g., 2× means IGP is twice as fast)\n",
    "fit_factor = chol_fit_time / iter_fit_time if iter_fit_time != 0 else float('inf')\n",
    "pred_factor = chol_pred_time / iter_pred_time if iter_pred_time != 0 else float('inf')\n",
    "total_factor = chol_total_time / iter_total_time if iter_total_time != 0 else float('inf')\n",
    "\n",
    "# ANSI escape codes for color formatting\n",
    "RED = \"\\033[91m\"\n",
    "GREEN = \"\\033[92m\"\n",
    "RESET = \"\\033[0m\"\n",
    "\n",
    "def format_speedup(speedup):\n",
    "    sign = \"+\" if speedup >= 0 else \"-\"\n",
    "    color = GREEN if speedup >= 0 else RED\n",
    "    return f\"{color}{sign}{abs(speedup):.2f}%{RESET}\"\n",
    "\n",
    "print(\"=== PERFORMANCE METRICS (Baseline: Cholesky GP) ===\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f\"Cholesky GP Fit Time      : {chol_fit_time:.4f} sec  (Baseline)\")\n",
    "print(f\"Iterative GP Fit Time     : {iter_fit_time:.4f} sec\")\n",
    "print(f\"Fit Time Difference       : {chol_fit_time - iter_fit_time:.4f} sec  ({format_speedup(fit_speedup)})\")\n",
    "print(f\"Speedup Factor            : {fit_factor:.2f}×\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f\"Cholesky GP Prediction Time  : {chol_pred_time:.4f} sec  (Baseline)\")\n",
    "print(f\"Iterative GP Prediction Time : {iter_pred_time:.4f} sec\")\n",
    "print(f\"Prediction Time Diff         : {chol_pred_time - iter_pred_time:.4f} sec  ({format_speedup(pred_speedup)})\")\n",
    "print(f\"Speedup Factor               : {pred_factor:.2f}×\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f\"Cholesky GP Total Time   : {chol_total_time:.4f} sec  (Baseline)\")\n",
    "print(f\"Iterative GP Total Time  : {iter_total_time:.4f} sec\")\n",
    "print(f\"Total Time Difference    : {chol_total_time - iter_total_time:.4f} sec  ({format_speedup(total_speedup)})\")\n",
    "print(f\"Speedup Factor           : {total_factor:.2f}×\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "\n",
    "# Accuracy Metrics\n",
    "print(\"=== ACCURACY METRICS ===\")\n",
    "print(f\"Cholesky GP RMSE         : {chol_rmse:.4f}  (Baseline)\")\n",
    "print(f\"Iterative GP RMSE        : {iter_rmse:.4f}\")\n",
    "print(f\"RMSE Difference          : {iter_rmse - chol_rmse:.4f}\")\n",
    "print(\"------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Iterative Gaussian Process (IGP)\n",
      "torch.Size([1659, 15])\n",
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 0.112502 s\n",
      "File: C:\\Users\\fredw\\AppData\\Local\\Temp\\ipykernel_35380\\504392267.py\n",
      "Function: train2 at line 6\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     6                                           def train2(model, train_x, train_y, test_x, test_y, training_iterations=100, lr=0.1):\n",
      "     7         1    1125018.0    1e+06    100.0      model.fit(train_x, train_y)\n",
      "     8                                               # optimizer = torch.optim.Adam([\n",
      "     9                                               #     {'params': model.kernel.parameters()}, \n",
      "    10                                               #     {'params': [model.noise.u]}  # Use raw_value instead of noise()\n",
      "    11                                               # ], lr=lr)\n",
      "    12                                               # runtime_log, mll_loss_log, test_rmse_log = [], [], []\n",
      "    13                                               # for i in tqdm(range(training_iterations)):\n",
      "    14                                               #     start_time = time.time()\n",
      "    15                                               #     optimizer.zero_grad()\n",
      "    16                                                   \n",
      "    17                                               #     model.fit(train_x, train_y)\n",
      "    18                                               #     loss = model.compute_mll(train_y)\n",
      "    19                                               #     loss.backward()\n",
      "    20                                               #     optimizer.step()\n",
      "    21                                               #     # scheduler.step()  # Update the learning rate\n",
      "    22                                           \n",
      "    23                                               #     # print(loss)\n",
      "    24                                               #     mean,covar = model.predict(test_x)\n",
      "    25                                               #     total_time = time.time() - start_time\n",
      "    26                                               #     runtime_log.append(total_time)\n",
      "    27                                               #     mll_loss_log.append(-loss.item())\n",
      "    28                                                   \n",
      "    29                                               #     test_rmse = (torch.mean(torch.abs(mean.detach().cpu() - test_y))).item()\n",
      "    30                                               #     test_rmse_log.append(test_rmse)\n",
      "    31                                               #     if (i + 1) % 20 == 0:\n",
      "    32                                               #         print(f'Iter {i+1}/{training_iterations}, Loss: {loss.item():.4f}')\n",
      "    33                                               \n",
      "    34                                               # return model, runtime_log, mll_loss_log, test_rmse_log, mean, covar\n",
      "\n",
      "Total time: 0.116043 s\n",
      "File: C:\\Users\\fredw\\AppData\\Local\\Temp\\ipykernel_35380\\504392267.py\n",
      "Function: run_igp_training at line 37\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    37                                           def run_igp_training():\n",
      "    38                                               # Set up the base and scaled kernel for the IGP\n",
      "    39         2       4230.0   2115.0      0.4      base_kernel_igp = MaternKernel(ard_num_dims=train_x.shape[-1], \n",
      "    40         1       3110.0   3110.0      0.3                                     lengthscale_prior=GammaPrior(-3.0, 3.0), \n",
      "    41         1          2.0      2.0      0.0                                     nu=1.5)\n",
      "    42         1      19685.0  19685.0      1.7      kernel_igp = ScaleKernel(base_kernel_igp, outputscale_prior=GammaPrior(-3.0, 3)).to(\"cuda:0\")\n",
      "    43                                               \n",
      "    44                                               # Initialize the Iterative Gaussian Process model with the desired parameters\n",
      "    45         2       7527.0   3763.5      0.6      igp_model = IterativeGaussianProcess(\n",
      "    46         1          2.0      2.0      0.0          kernel=kernel_igp, \n",
      "    47         1          2.0      2.0      0.0          noise=0.4, \n",
      "    48         1          1.0      1.0      0.0          dtype=global_dtype, \n",
      "    49         1          2.0      2.0      0.0          device=device,\n",
      "    50         1          2.0      2.0      0.0          cg_tol=1e-3, \n",
      "    51         1          1.0      1.0      0.0          cg_max_iter=20, \n",
      "    52         1          1.0      1.0      0.0          warm_start=True, \n",
      "    53         1          1.0      1.0      0.0          num_probes=16,\n",
      "    54         1          2.0      2.0      0.0          precon_type=\"piv_chol\", \n",
      "    55         1          1.0      1.0      0.0          trace_backend=\"Hutch\",\n",
      "    56         1          1.0      1.0      0.0          verbose=False, \n",
      "    57         1          1.0      1.0      0.0          track_iterations=True, \n",
      "    58         1         13.0     13.0      0.0          pred_lanczos_rank=train_x.shape[0],\n",
      "    59         1          1.0      1.0      0.0          compute_covariance=False\n",
      "    60                                               )\n",
      "    61                                               \n",
      "    62         1          3.0      3.0      0.0      training_iterations = 1\n",
      "    63         1          2.0      2.0      0.0      lr = 0.01\n",
      "    64                                           \n",
      "    65         1        770.0    770.0      0.1      print(\"Training Iterative Gaussian Process (IGP)\")\n",
      "    66                                               # Train the IGP model and capture relevant outputs2\n",
      "    67         1    1125062.0    1e+06     97.0      results = train2(igp_model, train_x, train_y, test_x, test_y, training_iterations, lr)\n",
      "    68         1         10.0     10.0      0.0      return results\n",
      "\n",
      "Total time: 0.0668925 s\n",
      "File: c:\\Users\\fredw\\chris\\Research\\randomGP2\\code\\gps.py\n",
      "Function: get_preconditioner at line 143\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   143                                               def get_preconditioner(self, X):\n",
      "   144         1          4.0      4.0      0.0          if self.precon_type == \"identity\":\n",
      "   145                                                       self.pinv_closure = identity_precon\n",
      "   146                                                       n = X.shape[0]\n",
      "   147                                                       self.preconditioner_matrix = torch.eye(n, dtype=self.dtype, device=self.device)\n",
      "   148                                                       self.preconditioner_matrix.requires_grad_(True)  # Ensure it tracks gradients   \n",
      "   149         1          3.0      3.0      0.0          elif self.precon_type == \"piv_chol\":\n",
      "   150                                                       # build_cholesky now returns (precond_inv, preconditioner_matrix, L_k)\n",
      "   151         2     666889.0 333444.5     99.7              self.pinv_closure, self.preconditioner_matrix, self.L_k = build_cholesky(\n",
      "   152         1       1982.0   1982.0      0.3                  X, self.kernel, self.noise.get_value(), self.precon_rank\n",
      "   153                                                       )\n",
      "   154         1         47.0     47.0      0.0              self.preconditioner_matrix.requires_grad_(True)  # Ensure it tracks gradients\n",
      "   155                                           \n",
      "   156                                                   else:\n",
      "   157                                                       raise NotImplementedError(f\"Preconditioner '{self.precon_type}' is not implemented.\")\n",
      "\n",
      "Total time: 0.112252 s\n",
      "File: c:\\Users\\fredw\\chris\\Research\\randomGP2\\code\\gps.py\n",
      "Function: fit at line 175\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   175                                               def fit(self, X, y):\n",
      "   176         1        183.0    183.0      0.0          self.X_train = X.to(dtype=self.dtype, device=self.device)\n",
      "   177         1         32.0     32.0      0.0          y = y.to(dtype=self.dtype, device=self.device)\n",
      "   178         1         17.0     17.0      0.0          n = self.X_train.shape[0]\n",
      "   179                                           \n",
      "   180                                                   # Precompute the kernel matrix once and cache it\n",
      "   181                                                   # with torch.no_grad():\n",
      "   182                                           \n",
      "   183         1     668985.0 668985.0     59.6          self.get_preconditioner(self.X_train)\n",
      "   184                                           \n",
      "   185         1         10.0     10.0      0.0          if self.K_train ==None:\n",
      "   186         1      46753.0  46753.0      4.2              self.K_train = self.kernel(self.X_train, self.X_train).evaluate()\n",
      "   187         1         29.0     29.0      0.0              self.K_train.requires_grad_(True)  # Ensure it tracks gradients\n",
      "   188                                           \n",
      "   189         1          9.0      9.0      0.0          def matmul_closure(b):\n",
      "   190                                                       return self.K_train @ b + self.noise.get_value()**2 * b\n",
      "   191                                           \n",
      "   192                                           \n",
      "   193                                                   #-------- Setup RHS of mbCG -------- \n",
      "   194         1        206.0    206.0      0.0          b = torch.empty((n, self.num_probes + 1), dtype=self.dtype, device=self.device)\n",
      "   195         1        508.0    508.0      0.0          b[:, 0] = y.view(-1)\n",
      "   196                                           \n",
      "   197         1         10.0     10.0      0.0          if self.num_probes > 0:\n",
      "   198         1          5.0      5.0      0.0              if self.precon_type == \"piv_chol\":\n",
      "   199         1        458.0    458.0      0.0                  print(self.L_k.shape)\n",
      "   200         1         21.0     21.0      0.0                  k = self.L_k.shape[1]\n",
      "   201                                                           # Sample z1 ~ NORMAL(0, I_k) and z2 ~ NORMAL(0, I_n)\n",
      "   202         1        366.0    366.0      0.0                  z1 = torch.randn((k, self.num_probes), dtype=self.dtype, device=self.device)\n",
      "   203         1        264.0    264.0      0.0                  z2 = torch.randn((n, self.num_probes), dtype=self.dtype, device=self.device)\n",
      "   204         1        508.0    508.0      0.0                  sigma = self.noise.get_value()\n",
      "   205                                                           # Form random probe vectors via reparameterization: L_k @ z1 + sigma * z2\n",
      "   206         1       1056.0   1056.0      0.1                  rand_vectors = self.L_k @ z1 + sigma * z2\n",
      "   207                                                       else:\n",
      "   208                                                           rand_vectors = torch.randn(n, self.num_probes, dtype=self.dtype, device=self.device)\n",
      "   209                                                       \n",
      "   210                                                       #---- Normalize to lie on the unit sphere ----\n",
      "   211         1        745.0    745.0      0.1              norms = torch.norm(rand_vectors, p=2, dim=0, keepdim=True)\n",
      "   212         1        689.0    689.0      0.1              rand_vectors = rand_vectors / (norms + 1e-10)\n",
      "   213         1        430.0    430.0      0.0              b[:, 1:] = rand_vectors\n",
      "   214                                           \n",
      "   215                                                   # Normalize each column of b to unit norm\n",
      "   216         1        480.0    480.0      0.0          rhs_norm = torch.norm(b, p=2, dim=0, keepdim=True)\n",
      "   217         1        340.0    340.0      0.0          rhs_is_zero = rhs_norm.lt(1e-10)\n",
      "   218         1        552.0    552.0      0.0          rhs_norm = rhs_norm.masked_fill(rhs_is_zero, 1)\n",
      "   219         1        272.0    272.0      0.0          b_normalized = b / rhs_norm\n",
      "   220                                           \n",
      "   221         1        382.0    382.0      0.0          self.Z = b[:, 1:]\n",
      "   222                                                   #---- Warm start initialization ----\n",
      "   223         1          7.0      7.0      0.0          if self.warm_start and self.previous_solutions is not None:\n",
      "   224                                                       x0 = self.previous_solutions\n",
      "   225                                                   else:\n",
      "   226         1        308.0    308.0      0.0              x0 = torch.zeros_like(b_normalized)\n",
      "   227                                           \n",
      "   228         1         17.0     17.0      0.0          if self.track_iterations and not hasattr(self, 'Us'):\n",
      "   229                                                       self.initialize_trackers()\n",
      "   230                                           \n",
      "   231                                                   #---- Build preconditioner/state ----\n",
      "   232         2     102250.0  51125.0      9.1          state, aux = initialize_cg(\n",
      "   233         1          3.0      3.0      0.0              matmul_closure, b_normalized, stop_updating_after=1e-10,\n",
      "   234         1          3.0      3.0      0.0              eps=1e-10, preconditioner=self.pinv_closure\n",
      "   235                                                   )\n",
      "   236         1         22.0     22.0      0.0          x0, has_converged, r0, batch_shape, residual_norm = state\n",
      "   237         1          4.0      4.0      0.0          (p0, gamma0, mul_storage, beta, alpha, is_zero, z0) = aux\n",
      "   238         1          8.0      8.0      0.0          if self.verbose:\n",
      "   239                                                       self.update_trackers(x0, r0, gamma0, p0, k=0)\n",
      "   240                                           \n",
      "   241                                                   #-------- CG iteration --------\n",
      "   242         1          3.0      3.0      0.0          alpha_history_list = []\n",
      "   243         1          3.0      3.0      0.0          beta_history_list = []\n",
      "   244         1          5.0      5.0      0.0          iter_idx = 0\n",
      "   245                                               \n",
      "   246        20        117.0      5.8      0.0          for k in range(1, self.cg_max_iter):\n",
      "   247        19      34140.0   1796.8      3.0              Ap0 = matmul_closure(p0)\n",
      "   248        38     178691.0   4702.4     15.9              x0, r0, gamma0, beta, p0, alpha = take_cg_step(\n",
      "   249        19         75.0      3.9      0.0                  Ap0, x0, r0, gamma0, p0, alpha, beta, z0, mul_storage,\n",
      "   250        19        101.0      5.3      0.0                  has_converged, eps=1e-10, is_zero=is_zero, precon=self.pinv_closure\n",
      "   251                                                       )\n",
      "   252                                                       # Accumulate the sliced vectors (ignoring the first column)\n",
      "   253        19       4156.0    218.7      0.4              alpha_history_list.append(alpha[0, 1:])\n",
      "   254        19       2505.0    131.8      0.2              beta_history_list.append(beta[0, 1:])\n",
      "   255        19        135.0      7.1      0.0              iter_idx += 1\n",
      "   256                                           \n",
      "   257        38      66157.0   1741.0      5.9              if cond_fn(k, self.cg_max_iter, self.cg_tol, r0, has_converged,\n",
      "   258        19         69.0      3.6      0.0                      residual_norm, stop_updating_after=1e-10, rhs_is_zero=rhs_is_zero):\n",
      "   259                                                           break\n",
      "   260        19        152.0      8.0      0.0              if self.verbose:\n",
      "   261                                                           print_analysis(k, alpha, residual_norm, gamma0, beta)\n",
      "   262                                                           self.update_trackers(x0, r0, gamma0, p0, k)\n",
      "   263                                           \n",
      "   264                                                 # Stack the accumulated rows to create history tensors.\n",
      "   265         1        588.0    588.0      0.1          alpha_history = torch.stack(alpha_history_list, dim=0)\n",
      "   266         1        391.0    391.0      0.0          beta_history = torch.stack(beta_history_list, dim=0)\n",
      "   267                                           \n",
      "   268                                                   # Transpose the history tensors so the probe dimension comes first.\n",
      "   269                                                   # Now alpha_batch has shape (num_probes, iter_idx) and beta_history (num_probes, iter_idx).\n",
      "   270         1         97.0     97.0      0.0          alpha_batch = alpha_history.transpose(0, 1)  # shape: (num_probes, iter_idx)\n",
      "   271         1         53.0     53.0      0.0          beta_batch = beta_history.transpose(0, 1)    # shape: (num_probes, iter_idx)\n",
      "   272                                                   # For each probe, beta should be of length (iter_idx - 1) (since beta_list[0] corresponds to iteration 2 onward).\n",
      "   273         1        197.0    197.0      0.0          beta_batch = beta_batch[:, :alpha_batch.size(1) - 1]  # shape: (num_probes, iter_idx - 1)\n",
      "   274                                                   # Build a batch of Lanczos matrices.\n",
      "   275         1       6395.0   6395.0      0.6          T_batch = build_lanczos_batched(alpha_batch, beta_batch, dtype=self.dtype, device=self.device, eps=1e-16)\n",
      "   276                                                   # T_batch now has shape (num_probes, iter_idx, iter_idx)\n",
      "   277                                                   # If needed, convert the batch into a list of individual matrices.\n",
      "   278        17       1372.0     80.7      0.1          self.lanczos_iterates = [T_batch[i] for i in range(T_batch.size(0))]\n",
      "   279                                           \n",
      "   280         1        343.0    343.0      0.0          solution = x0 * rhs_norm\n",
      "   281         1        220.0    220.0      0.0          self.previous_solution = solution  # Cache for warm start\n",
      "   282         1        317.0    317.0      0.0          self.alpha = solution[:, 0]\n",
      "   283         1        308.0    308.0      0.0          self.probe_solutions = solution[:, 1:]\n",
      "\n",
      "Total time: 0.0174848 s\n",
      "File: c:\\Users\\fredw\\chris\\Research\\randomGP2\\code\\mbcg.py\n",
      "Function: take_cg_step at line 225\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   225                                           def take_cg_step(Ap0, x0, r0, gamma0, p0, alpha, beta, z0, mul_storage, has_converged, eps, is_zero, precon):\n",
      "   226        19      10214.0    537.6      5.8      dot = torch.sum(p0 * Ap0, dim=-2, keepdim=True)\n",
      "   227        19       4531.0    238.5      2.6      is_small = dot < eps\n",
      "   228        19      13821.0    727.4      7.9      alpha_new = gamma0 / torch.where(is_small, torch.ones_like(dot), dot)\n",
      "   229        19      16065.0    845.5      9.2      alpha_new = torch.where(is_small | has_converged, torch.zeros_like(alpha_new), alpha_new)\n",
      "   230        19       9485.0    499.2      5.4      r_new = r0 - alpha_new * Ap0\n",
      "   231        19       9844.0    518.1      5.6      x_new = x0 + alpha_new * p0\n",
      "   232        19      61696.0   3247.2     35.3      precond_residual = precon(r_new)\n",
      "   233        19      10109.0    532.1      5.8      new_gamma = torch.sum(r_new * precond_residual, dim=-2, keepdim=True)\n",
      "   234        19       5231.0    275.3      3.0      is_small_gamma = gamma0 < eps\n",
      "   235        19      14238.0    749.4      8.1      beta_new = new_gamma / torch.where(is_small_gamma, torch.ones_like(gamma0), gamma0)\n",
      "   236        19       9142.0    481.2      5.2      beta_new = torch.where(is_small_gamma, torch.zeros_like(beta_new), beta_new)\n",
      "   237        19      10163.0    534.9      5.8      p_new = precond_residual + beta_new * p0\n",
      "   238        19        309.0     16.3      0.2      return x_new, r_new, new_gamma, beta_new, p_new, alpha_new\n",
      "\n",
      "Total time: 0.0654542 s\n",
      "File: c:\\Users\\fredw\\chris\\Research\\randomGP2\\code\\preconditioners.py\n",
      "Function: _pivoted_cholesky at line 13\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    13                                           def _pivoted_cholesky(x, kernel_fn, outputscale: float, rank: int = 3):\n",
      "    14         1         13.0     13.0      0.0      n = x.shape[0]\n",
      "    15         1        257.0    257.0      0.0      L = torch.zeros((rank, n), device=x.device, dtype=x.dtype)\n",
      "    16         1        638.0    638.0      0.1      d = (outputscale ** 2) * torch.ones(n, device=x.device, dtype=x.dtype)\n",
      "    17         1        185.0    185.0      0.0      pi = torch.arange(n, device=x.device)\n",
      "    18                                               \n",
      "    19        16        152.0      9.5      0.0      for m in range(rank):\n",
      "    20                                                   # Find the pivot index in the unsampled region.\n",
      "    21        15      41825.0   2788.3      6.4          pivot_relative = torch.argmax(d[m:]).item()\n",
      "    22        15        109.0      7.3      0.0          i = m + pivot_relative\n",
      "    23                                                   \n",
      "    24                                                   # Swap indices in pi: use local variables to minimize overhead.\n",
      "    25        15       1421.0     94.7      0.2          pivot_index = pi[m]\n",
      "    26        15       5491.0    366.1      0.8          pi[m] = pi[i]\n",
      "    27        15       3255.0    217.0      0.5          pi[i] = pivot_index\n",
      "    28                                                   \n",
      "    29                                                   # Update L at the pivot location.\n",
      "    30        15       1037.0     69.1      0.2          current_idx = pi[m]\n",
      "    31        15      45255.0   3017.0      6.9          L[m, current_idx] = torch.sqrt(d[current_idx])\n",
      "    32                                                   \n",
      "    33                                                   # Compute kernel between current pivot and the rest.\n",
      "    34                                                   # Note: kernel_fn must support TorchScript for full benefit.\n",
      "    35        15      32638.0   2175.9      5.0          a = kernel_fn(x[current_idx].unsqueeze(0), x[pi[m+1:]])\n",
      "    36        15     363773.0  24251.5     55.6          a = a.squeeze(0)\n",
      "    37                                                   \n",
      "    38        15        107.0      7.1      0.0          if m > 0:\n",
      "    39                                                       # Use pre-sliced pivot indices to reduce indexing overhead.\n",
      "    40        14      28456.0   2032.6      4.3              prev_L = L[:m, pi[m]]\n",
      "    41        14       9129.0    652.1      1.4              rest_L = L[:m, pi[m+1:]]\n",
      "    42        14      11835.0    845.4      1.8              correction = torch.sum(prev_L.unsqueeze(0).T * rest_L, dim=0)\n",
      "    43                                                   else:\n",
      "    44         1          5.0      5.0      0.0              correction = 0.0\n",
      "    45                                                   \n",
      "    46                                                   # Compute the new column of L.\n",
      "    47        15      74227.0   4948.5     11.3          l = (a - correction) / L[m, current_idx]\n",
      "    48        15      11355.0    757.0      1.7          L[m, pi[m+1:]] = l\n",
      "    49                                                   \n",
      "    50                                                   # Update the diagonal elements for the unsampled indices.\n",
      "    51        15      23358.0   1557.2      3.6          d[pi[m+1:]] -= l ** 2\n",
      "    52         1         21.0     21.0      0.0      return L\n",
      "\n",
      "Total time: 0.0666221 s\n",
      "File: c:\\Users\\fredw\\chris\\Research\\randomGP2\\code\\preconditioners.py\n",
      "Function: build_cholesky at line 59\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    59                                           def build_cholesky(X, kernel, noise, rank):\n",
      "    60         1     658321.0 658321.0     98.8      L = _pivoted_cholesky(X, kernel, kernel.outputscale, rank=rank)\n",
      "    61         1        282.0    282.0      0.0      noise_inv2 = noise ** -2\n",
      "    62         1        118.0    118.0      0.0      noise_inv4 = noise ** -4\n",
      "    63         1       1936.0   1936.0      0.3      M = torch.eye(rank, device=L.device, dtype=L.dtype) + noise_inv2 * (L @ L.T)\n",
      "    64         1       3653.0   3653.0      0.5      M_cho_factor = psd_safe_cholesky(M)\n",
      "    65         1         15.0     15.0      0.0      def precond_inv(v):\n",
      "    66                                                   if v.ndim == 1:\n",
      "    67                                                       v = v.unsqueeze(1)\n",
      "    68                                                   z = torch.cholesky_solve(L @ v, M_cho_factor, upper=False)\n",
      "    69                                                   result = noise_inv2 * v - noise_inv4 * (L.T @ z)\n",
      "    70                                                   return result.squeeze(-1) if result.ndim == 2 and result.shape[1] == 1 else result\n",
      "    71         1         21.0     21.0      0.0      n = X.shape[0]\n",
      "    72         1       1799.0   1799.0      0.3      A = noise ** 2 * torch.eye(n, device=X.device, dtype=X.dtype) + (L.T @ L)\n",
      "    73         1         76.0     76.0      0.0      return precond_inv, A, L.T #builds backwards from pivoted :<\n",
      "\n",
      "Total time: 0 s\n",
      "File: c:\\Users\\fredw\\miniconda3\\envs\\softki\\Lib\\site-packages\\torch\\nn\\modules\\module.py\n",
      "Function: _forward_unimplemented at line 383\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   383                                           def _forward_unimplemented(self, *input: Any) -> None:\n",
      "   384                                               r\"\"\"Define the computation performed at every call.\n",
      "   385                                           \n",
      "   386                                               Should be overridden by all subclasses.\n",
      "   387                                           \n",
      "   388                                               .. note::\n",
      "   389                                                   Although the recipe for forward pass needs to be defined within\n",
      "   390                                                   this function, one should call the :class:`Module` instance afterwards\n",
      "   391                                                   instead of this since the former takes care of running the\n",
      "   392                                                   registered hooks while the latter silently ignores them.\n",
      "   393                                               \"\"\"\n",
      "   394                                               raise NotImplementedError(\n",
      "   395                                                   f'Module [{type(self).__name__}] is missing the required \"forward\" function'\n",
      "   396                                               )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from line_profiler import LineProfiler\n",
    "from preconditioners import build_cholesky,identity_precon\n",
    "\n",
    "def train2(model, train_x, train_y, test_x, test_y, training_iterations=100, lr=0.1):\n",
    "    model.fit(train_x, train_y)\n",
    "    # optimizer = torch.optim.Adam([\n",
    "    #     {'params': model.kernel.parameters()}, \n",
    "    #     {'params': [model.noise.u]}  # Use raw_value instead of noise()\n",
    "    # ], lr=lr)\n",
    "    # runtime_log, mll_loss_log, test_rmse_log = [], [], []\n",
    "    # for i in tqdm(range(training_iterations)):\n",
    "    #     start_time = time.time()\n",
    "    #     optimizer.zero_grad()\n",
    "        \n",
    "    #     model.fit(train_x, train_y)\n",
    "    #     loss = model.compute_mll(train_y)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     # scheduler.step()  # Update the learning rate\n",
    "\n",
    "    #     # print(loss)\n",
    "    #     mean,covar = model.predict(test_x)\n",
    "    #     total_time = time.time() - start_time\n",
    "    #     runtime_log.append(total_time)\n",
    "    #     mll_loss_log.append(-loss.item())\n",
    "        \n",
    "    #     test_rmse = (torch.mean(torch.abs(mean.detach().cpu() - test_y))).item()\n",
    "    #     test_rmse_log.append(test_rmse)\n",
    "    #     if (i + 1) % 20 == 0:\n",
    "    #         print(f'Iter {i+1}/{training_iterations}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # return model, runtime_log, mll_loss_log, test_rmse_log, mean, covar\n",
    "\n",
    "\n",
    "def run_igp_training():\n",
    "    # Set up the base and scaled kernel for the IGP\n",
    "    base_kernel_igp = MaternKernel(ard_num_dims=train_x.shape[-1], \n",
    "                                   lengthscale_prior=GammaPrior(-3.0, 3.0), \n",
    "                                   nu=1.5)\n",
    "    kernel_igp = ScaleKernel(base_kernel_igp, outputscale_prior=GammaPrior(-3.0, 3)).to(\"cuda:0\")\n",
    "    \n",
    "    # Initialize the Iterative Gaussian Process model with the desired parameters\n",
    "    igp_model = IterativeGaussianProcess(\n",
    "        kernel=kernel_igp, \n",
    "        noise=0.4, \n",
    "        dtype=global_dtype, \n",
    "        device=device,\n",
    "        cg_tol=1e-3, \n",
    "        cg_max_iter=20, \n",
    "        warm_start=True, \n",
    "        num_probes=16,\n",
    "        precon_type=\"piv_chol\", \n",
    "        trace_backend=\"Hutch\",\n",
    "        verbose=False, \n",
    "        track_iterations=True, \n",
    "        pred_lanczos_rank=train_x.shape[0],\n",
    "        compute_covariance=False\n",
    "    )\n",
    "    \n",
    "    training_iterations = 1\n",
    "    lr = 0.01\n",
    "\n",
    "    print(\"Training Iterative Gaussian Process (IGP)\")\n",
    "    # Train the IGP model and capture relevant outputs2\n",
    "    results = train2(igp_model, train_x, train_y, test_x, test_y, training_iterations, lr)\n",
    "    return results\n",
    "\n",
    "# Set up the line profiler\n",
    "lp = LineProfiler()\n",
    "from mbcg import take_cg_step\n",
    "from preconditioners import _pivoted_cholesky\n",
    "# Add the main training function and key IGP class methods you want to profile.\n",
    "# Here we add the train function, and as an example, the forward method of IterativeGaussianProcess.\n",
    "lp.add_function(train2)\n",
    "lp.add_function(take_cg_step)\n",
    "lp.add_function(build_cholesky)\n",
    "lp.add_function(_pivoted_cholesky)\n",
    "\n",
    "lp.add_function(IterativeGaussianProcess.forward)\n",
    "lp.add_function(IterativeGaussianProcess.fit)\n",
    "lp.add_function(IterativeGaussianProcess.get_preconditioner)\n",
    "\n",
    "# If there are other important methods (for example, an 'optimize' method), add them similarly:\n",
    "# lp.add_function(IterativeGaussianProcess.optimize)\n",
    "\n",
    "# Wrap the IGP training run with the profiler and execute it.\n",
    "lp_wrapper = lp(run_igp_training)\n",
    "results = lp_wrapper()\n",
    "\n",
    "# Print out the profiling statistics.\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 852.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 25.57 GiB is allocated by PyTorch, and 377.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 57\u001b[0m\n\u001b[0;32m     50\u001b[0m igp \u001b[38;5;241m=\u001b[39m IterativeGaussianProcess(kernel\u001b[38;5;241m=\u001b[39mkernel, noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mglobal_dtype, device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     51\u001b[0m                                cg_tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, cg_max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, warm_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_probes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     52\u001b[0m                                precon_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpiv_chol\u001b[39m\u001b[38;5;124m\"\u001b[39m, trace_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHutch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     53\u001b[0m                                verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, track_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m     54\u001b[0m                                pred_lanczos_rank\u001b[38;5;241m=\u001b[39mtrain_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], compute_covariance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Train the iterative GP model\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m model_igp, runtime_log_igp, mll_loss_log_igp, test_rmse_log_igp, mean_igp, covar_igp \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43migp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Train the Cholesky-based GP model\u001b[39;00m\n\u001b[0;32m     60\u001b[0m model_cgp, runtime_log_cgp, mll_loss_log_cgp, test_rmse_log_cgp, mean_cgp, covar_cgp \u001b[38;5;241m=\u001b[39m train(cgp, train_x, train_y, test_x, test_y, training_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_x, train_y, test_x, test_y, training_iterations, lr)\u001b[0m\n\u001b[0;32m     16\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 19\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_mll(train_y)\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\fredw\\chris\\Research\\randomGP2\\code\\gps.py:188\u001b[0m, in \u001b[0;36mIterativeGaussianProcess.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmatmul_closure\u001b[39m(b):\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK_train \u001b[38;5;241m@\u001b[39m b \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise\u001b[38;5;241m.\u001b[39mget_value()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m b\n\u001b[1;32m--> 188\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_preconditioner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m#-------- Setup RHS of mbCG -------- \u001b[39;00m\n\u001b[0;32m    191\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_probes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\fredw\\chris\\Research\\randomGP2\\code\\gps.py:151\u001b[0m, in \u001b[0;36mIterativeGaussianProcess.get_preconditioner\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreconditioner_matrix\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Ensure it tracks gradients   \u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecon_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpiv_chol\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# build_cholesky now returns (precond_inv, preconditioner_matrix, L_k)\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_closure, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreconditioner_matrix, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL_k \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_cholesky\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecon_rank\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreconditioner_matrix\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Ensure it tracks gradients\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\fredw\\chris\\Research\\randomGP2\\code\\preconditioners.py:54\u001b[0m, in \u001b[0;36mbuild_cholesky\u001b[1;34m(x, kernel, noise, rank)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Squeeze if needed\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m v\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m result\n\u001b[1;32m---> 54\u001b[0m preconditioner_matrix \u001b[38;5;241m=\u001b[39m L \u001b[38;5;241m@\u001b[39m L\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m \u001b[43mnoise\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m precond_inv, preconditioner_matrix, L\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 852.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 25.57 GiB is allocated by PyTorch, and 377.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_x, train_y, test_x, test_y, training_iterations=100, lr=0.1):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.kernel.parameters()}, \n",
    "        {'params': [model.noise.u]}  # Use raw_value instead of noise()\n",
    "    ], lr=lr)\n",
    "    \n",
    "    runtime_log, mll_loss_log, test_rmse_log = [], [], []\n",
    "    \n",
    "    for i in tqdm(range(training_iterations)):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.fit(train_x, train_y)\n",
    "        loss = model.compute_mll(train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        mean, covar = model.predict(test_x)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        runtime_log.append(total_time)\n",
    "        mll_loss_log.append(-loss.item())  # Logging positive MLL value\n",
    "        \n",
    "        test_rmse = torch.mean(torch.abs(mean.detach().cpu()- test_y)).item()\n",
    "        test_rmse_log.append(test_rmse)\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f'Iter {i+1}/{training_iterations}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return model, runtime_log, mll_loss_log, test_rmse_log, mean, covar\n",
    "\n",
    "# Example usage:\n",
    "# Assuming that model_igp and model_cgp are instantiated GP models (Iterative GP and Cholesky GP respectively),\n",
    "# and train_x, train_y, test_x, test_y are already defined.\n",
    "base_kernel = MaternKernel(ard_num_dims=train_x.shape[-1], lengthscale_prior=GammaPrior(3.0, 6.0), nu=1.5)\n",
    "kernel = ScaleKernel(base_kernel, outputscale_prior=GammaPrior(2.0, 0.15)).to(device)\n",
    "\n",
    "gpr = CholeskyGaussianProcess(kernel=kernel, dtype=global_dtype, noise=0.4, device=device)\n",
    "\n",
    "\n",
    "base_kernel = MaternKernel(ard_num_dims=train_x.shape[-1], lengthscale_prior=GammaPrior(3.0, 6.0), nu=1.5)\n",
    "kernel = ScaleKernel(base_kernel, outputscale_prior=GammaPrior(2.0, 0.15)).to(device)\n",
    "\n",
    "igp = IterativeGaussianProcess(kernel=kernel, noise=0.4, dtype=global_dtype, device=device,\n",
    "                               cg_tol=1e-3, cg_max_iter=100, warm_start=True, num_probes=16,\n",
    "                               precon_type=\"piv_chol\", trace_backend=\"Hutch\",\n",
    "                               verbose=False, track_iterations=False, \n",
    "                               pred_lanczos_rank=train_x.shape[0], compute_covariance=False)\n",
    "                               \n",
    "# Train the iterative GP model\n",
    "model_igp, runtime_log_igp, mll_loss_log_igp, test_rmse_log_igp, mean_igp, covar_igp = train(igp, train_x, train_y, test_x, test_y, training_iterations=10, lr=0.01)\n",
    "\n",
    "# Train the Cholesky-based GP model\n",
    "model_cgp, runtime_log_cgp, mll_loss_log_cgp, test_rmse_log_cgp, mean_cgp, covar_cgp = train(cgp, train_x, train_y, test_x, test_y, training_iterations=10, lr=0.01)\n",
    "\n",
    "# ----------------------------\n",
    "# Two-Panel Plot: RMSE, MLL, and Runtime\n",
    "# ----------------------------\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "\n",
    "# Plot RMSE comparison: |μ - yₜᵣᵤₑ|\n",
    "axes[0].plot(test_rmse_log_igp, label=\"IGP Test RMSE\")\n",
    "axes[0].plot(test_rmse_log_cgp, label=\"CGP Test RMSE\")\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].set_ylabel(\"Test RMSE\")\n",
    "axes[0].set_title(r\"Test RMSE Comparison: $|\\mu - y_{\\text{true}}|$\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot MLL comparison: -𝓛 (where a lower negative loss indicates a higher likelihood)\n",
    "axes[1].plot(mll_loss_log_igp, label=\"IGP MLL\")\n",
    "axes[1].plot(mll_loss_log_cgp, label=\"CGP MLL\")\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "axes[1].set_ylabel(\"Marginal Log Likelihood (Positive)\")\n",
    "axes[1].set_title(r\"MLL Comparison: $-\\mathcal{L}$\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot runtime per iteration\n",
    "axes[2].plot(runtime_log_igp, label=\"IGP Runtime\")\n",
    "axes[2].plot(runtime_log_cgp, label=\"CGP Runtime\")\n",
    "axes[2].set_xlabel(\"Iteration\")\n",
    "axes[2].set_ylabel(\"Runtime (seconds)\")\n",
    "axes[2].set_title(r\"Runtime per Iteration\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_data(true_function, train_range=(-3, 3), test_range=(-3, 3), \n",
    "                  n_train=40, n_test=100, noise_std=0.1, dim=1,\n",
    "                  device='cuda:0', dtype=torch.float64):\n",
    "    # Generate training data: each sample is a vector in R^dim\n",
    "    low_train, high_train = train_range\n",
    "    X_train = torch.empty((n_train, dim), dtype=dtype, device=device).uniform_(low_train, high_train)\n",
    "    y_train = true_function(X_train) + noise_std * torch.randn(n_train, dtype=dtype, device=device)\n",
    "    \n",
    "    # Generate test data: samples drawn uniformly from the test range\n",
    "    low_test, high_test = test_range\n",
    "    X_test = torch.empty((n_test, dim), dtype=dtype, device=device).uniform_(low_test, high_test)\n",
    "    y_test = true_function(X_test)  # No noise added to test data\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Define the true function for high-dimensional input\n",
    "def true_function(x):\n",
    "    # x is assumed to be of shape (n, dim).\n",
    "    # For illustration, we define a function:\n",
    "    #   f(x) = \\sum_{i=1}^{dim} (\\sin(2 x_i) + \\cos(3 x_i))\n",
    "    # If x has shape (n, dim), the output is (n,)\n",
    "    return torch.sum(torch.sin(2 * x) + torch.cos(3 * x), dim=1)\n",
    "\n",
    "# Set global_dtype if needed, for example:\n",
    "global_dtype = torch.float64\n",
    "\n",
    "# Generate data with dim=5 for example\n",
    "train_x, train_y, test_x, test_y = generate_data(true_function,\n",
    "                                                  train_range=(-3, 3),\n",
    "                                                  test_range=(-5, 5),\n",
    "                                                  n_train=40,\n",
    "                                                  n_test=100,\n",
    "                                                  noise_std=0.1,\n",
    "                                                  dim=5,\n",
    "                                                  dtype=global_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_kernel = MaternKernel(ard_num_dims=train_x.shape[-1] , lengthscale_prior=GammaPrior(-3.0, 3.0),nu=1.5)\n",
    "kernel = ScaleKernel(base_kernel, outputscale_prior=GammaPrior(-3.0, 3)).to(\"cuda:0\")\n",
    "model = CholeskyGaussianProcess(kernel=kernel, dtype=global_dtype, noise=.4, device='cuda:0',compute_covariance=True)\n",
    "training_iterations=50\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_x, train_y, test_x, test_y, training_iterations=100, lr=0.1):\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.kernel.parameters()}, \n",
    "        {'params': [model.noise.u]}  # Use raw_value instead of noise()\n",
    "    ], lr=lr)\n",
    "    runtime_log, mll_loss_log, test_rmse_log = [], [], []\n",
    "    for i in tqdm(range(training_iterations)):\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.fit(train_x, train_y)\n",
    "        loss = model.compute_mll(train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update the learning rate\n",
    "\n",
    "        # print(loss)\n",
    "        mean, covar = model.predict(test_x)\n",
    "        # mean = model.predict(test_x)\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        runtime_log.append(total_time)\n",
    "        mll_loss_log.append(-loss.item())\n",
    "        \n",
    "        test_rmse = (torch.mean(torch.abs(mean - test_y))).item()\n",
    "        test_rmse_log.append(test_rmse)\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f'Iter {i+1}/{training_iterations}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return model, runtime_log, mll_loss_log, test_rmse_log, mean, covar\n",
    "train_x, train_y, test_x, test_y = generate_data(true_function,train_range=(-3, 3), test_range=(-3,3),n_train=2000, dtype=global_dtype)\n",
    "\n",
    "model, runtime_log, mll_loss_log, test_rmse_log, mean, covar = train(model, train_x, train_y, test_x, test_y, training_iterations,lr=.01)\n",
    "std = torch.sqrt(torch.diag(covar))\n",
    "# train_x, train_y, test_x, test_y = generate_data(true_function,train_range=(-3, 3), test_range=(-5,5), dtype=global_dtype,n_train=1000)\n",
    "# ----------------------------\n",
    "# Initialize the Iterative GP\n",
    "# ----------------------------\n",
    "base_kernel_igp = MaternKernel(ard_num_dims=train_x.shape[-1], \n",
    "                               lengthscale_prior=GammaPrior(-3.0, 3.0), \n",
    "                               nu=1.5)\n",
    "kernel_igp = ScaleKernel(base_kernel_igp, outputscale_prior=GammaPrior(-3.0, 3)).to(\"cuda:0\")\n",
    "igp_model = IterativeGaussianProcess(kernel=kernel_igp, noise=0.4, dtype=global_dtype, device=device,\n",
    "                                     cg_tol=1e-3, cg_max_iter=20, warm_start=False, num_probes=16,\n",
    "                                     precon_type=\"piv_chol\", trace_backend=\"Hutch\",\n",
    "                                     verbose=False, track_iterations=True, \n",
    "                                     pred_lanczos_rank=train_x.shape[0],compute_covariance=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Initialize the Cholesky GP\n",
    "# ----------------------------\n",
    "base_kernel_cgp = MaternKernel(ard_num_dims=train_x.shape[-1], \n",
    "                               lengthscale_prior=GammaPrior(-3.0, 3.0), \n",
    "                               nu=1.5)\n",
    "kernel_cgp = ScaleKernel(base_kernel_cgp, outputscale_prior=GammaPrior(-3.0, 3)).to(\"cuda:0\")\n",
    "cgp_model = CholeskyGaussianProcess(kernel=kernel_cgp, dtype=global_dtype, noise=0.4, device=\"cuda:0\")\n",
    "\n",
    "training_iterations = 50\n",
    "lr = 0.01\n",
    "\n",
    "print(\"Training Iterative Gaussian Process (IGP)\")\n",
    "igp_model, runtime_log_igp, mll_loss_log_igp, test_rmse_log_igp, mean_igp, covar_igp = \\\n",
    "    train(igp_model, train_x, train_y, test_x, test_y, training_iterations, lr)\n",
    "\n",
    "print(\"\\nTraining Cholesky Gaussian Process (CGP)\")\n",
    "cgp_model, runtime_log_cgp, mll_loss_log_cgp, test_rmse_log_cgp, mean_cgp, covar_cgp = \\\n",
    "    train(cgp_model, train_x, train_y, test_x, test_y, training_iterations, lr)\n",
    "\n",
    "# ----------------------------\n",
    "# Two-Panel Plot: RMSE and MLL\n",
    "# ----------------------------\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "\n",
    "# Plot RMSE comparison\n",
    "axes[0].plot(test_rmse_log_igp, label=\"IGP Test RMSE\")\n",
    "axes[0].plot(test_rmse_log_cgp, label=\"CGP Test RMSE\")\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].set_ylabel(\"Test RMSE\")\n",
    "axes[0].set_title(r\"Test RMSE Comparison: $|\\mu - y_{\\text{true}}|$\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot MLL comparison\n",
    "axes[1].plot(mll_loss_log_igp, label=\"IGP MLL\")\n",
    "axes[1].plot(mll_loss_log_cgp, label=\"CGP MLL\")\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "axes[1].set_ylabel(\"Marginal Log Likelihood (Positive)\")\n",
    "axes[1].set_title(r\"MLL Comparison: $-\\mathcal{L}$\")\n",
    "axes[1].legend()\n",
    "\n",
    "\n",
    "# Plot MLL comparison\n",
    "axes[2].plot(runtime_log_igp, label=\"IGP MLL\")\n",
    "axes[2].plot(runtime_log_cgp, label=\"CGP MLL\")\n",
    "axes[2].set_xlabel(\"Iteration\")\n",
    "axes[2].set_ylabel(\"Runtime\")\n",
    "axes[2].set_title(r\"time$\")\n",
    "axes[2].legend()\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# Optionally, Plot GP Predictions\n",
    "# ----------------------------\n",
    "# std_igp = torch.sqrt(torch.diag(covar_igp))\n",
    "std_igp = 0\n",
    "\n",
    "std_cgp = torch.sqrt(torch.diag(covar_cgp))\n",
    "\n",
    "plot_gpr_results(\n",
    "    train_x=train_x,\n",
    "    train_y=train_y,\n",
    "    test_x=test_x,\n",
    "    test_y=test_y,\n",
    "    GP_mean=mean_igp,\n",
    "    std=std_igp,\n",
    "    mll_loss_log=mll_loss_log_igp,\n",
    "    test_rmse_log=test_rmse_log_igp,\n",
    ")\n",
    "\n",
    "plot_gpr_results(\n",
    "    train_x=train_x,\n",
    "    train_y=train_y,\n",
    "    test_x=test_x,\n",
    "    test_y=test_y,\n",
    "    GP_mean=mean_cgp,\n",
    "    std=std_cgp,\n",
    "    mll_loss_log=mll_loss_log_cgp,\n",
    "    test_rmse_log=test_rmse_log_cgp,\n",
    ")\n",
    "\n",
    "# model, runtime_log, mll_loss_log, test_rmse_log, mean = train(model, train_x, train_y, test_x, test_y, training_iterations,lr=.01)\n",
    "# std=0\n",
    "plot_gpr_results(\n",
    "    train_x=train_x,\n",
    "    train_y=train_y,\n",
    "    test_x=test_x,\n",
    "    test_y=test_y,\n",
    "    GP_mean=mean,\n",
    "    std=std,\n",
    "    mll_loss_log=mll_loss_log,\n",
    "    test_rmse_log=test_rmse_log\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
