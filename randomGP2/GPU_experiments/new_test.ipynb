{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.236s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import random\n",
    "import unittest\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "from linear_operator.utils.linear_cg import linear_cg\n",
    "from linear_operator.utils.warnings import NumericalWarning\n",
    "\n",
    "\n",
    "class TestLinearCG(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        if os.getenv(\"UNLOCK_SEED\") is None or os.getenv(\"UNLOCK_SEED\").lower() == \"false\":\n",
    "            self.rng_state = torch.get_rng_state()\n",
    "            torch.manual_seed(0)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(0)\n",
    "            random.seed(0)\n",
    "\n",
    "    def tearDown(self):\n",
    "        if hasattr(self, \"rng_state\"):\n",
    "            torch.set_rng_state(self.rng_state)\n",
    "\n",
    "    def test_cg(self):\n",
    "        size = 100\n",
    "        matrix = torch.randn(size, size, dtype=torch.float64)\n",
    "        matrix = matrix.matmul(matrix.mT)\n",
    "        matrix.div_(matrix.norm())\n",
    "        matrix.add_(torch.eye(matrix.size(-1), dtype=torch.float64).mul_(1e-1))\n",
    "\n",
    "        # set up vector rhs\n",
    "        rhs = torch.randn(size, dtype=torch.float64)\n",
    "\n",
    "        # basic solve\n",
    "        solves = linear_cg(matrix.matmul, rhs=rhs, max_iter=size)\n",
    "\n",
    "        # solve with init value\n",
    "        init = torch.randn(size, dtype=torch.float64)\n",
    "        solves_with_init = linear_cg(matrix.matmul, rhs=rhs, max_iter=size, initial_guess=init)\n",
    "\n",
    "        # Check cg\n",
    "        matrix_chol = torch.linalg.cholesky(matrix)\n",
    "        actual = torch.cholesky_solve(rhs.unsqueeze(dim=1), matrix_chol).squeeze()\n",
    "        self.assertTrue(torch.allclose(solves, actual, atol=1e-3, rtol=1e-4))\n",
    "        self.assertTrue(torch.allclose(solves_with_init, actual, atol=1e-3, rtol=1e-4))\n",
    "\n",
    "        # set up matrix rhs\n",
    "        numcols = 50\n",
    "        rhs = torch.randn(size, numcols, dtype=torch.float64)\n",
    "\n",
    "        # basic solve\n",
    "        solves = linear_cg(matrix.matmul, rhs=rhs, max_iter=size)\n",
    "\n",
    "        # solve with init value\n",
    "        init = torch.randn(size, numcols, dtype=torch.float64)\n",
    "        solves_with_init = linear_cg(matrix.matmul, rhs=rhs, max_iter=size, initial_guess=init)\n",
    "\n",
    "        # Check cg\n",
    "        actual = torch.cholesky_solve(rhs, matrix_chol)\n",
    "        self.assertTrue(torch.allclose(solves, actual, atol=1e-3, rtol=1e-4))\n",
    "        self.assertTrue(torch.allclose(solves_with_init, actual, atol=1e-3, rtol=1e-4))\n",
    "\n",
    "    def test_cg_with_tridiag(self):\n",
    "        size = 10\n",
    "        matrix = torch.randn(size, size, dtype=torch.float64)\n",
    "        matrix = matrix.matmul(matrix.mT)\n",
    "        matrix.div_(matrix.norm())\n",
    "        matrix.add_(torch.eye(matrix.size(-1), dtype=torch.float64).mul_(1e-1))\n",
    "\n",
    "        rhs = torch.randn(size, 50, dtype=torch.float64)\n",
    "        with warnings.catch_warnings(record=True) as ws:\n",
    "            solves, t_mats = linear_cg(\n",
    "                matrix.matmul,\n",
    "                rhs=rhs,\n",
    "                n_tridiag=5,\n",
    "                max_tridiag_iter=10,\n",
    "                max_iter=size,\n",
    "                tolerance=0,\n",
    "                eps=1e-15,\n",
    "            )\n",
    "            self.assertTrue(any(issubclass(w.category, NumericalWarning) for w in ws))\n",
    "\n",
    "        # Check cg\n",
    "        matrix_chol = torch.linalg.cholesky(matrix)\n",
    "        actual = torch.cholesky_solve(rhs, matrix_chol)\n",
    "        self.assertTrue(torch.allclose(solves, actual, atol=1e-3, rtol=1e-4))\n",
    "\n",
    "        # Check tridiag\n",
    "        eigs = torch.linalg.eigvalsh(matrix)\n",
    "        for i in range(5):\n",
    "            approx_eigs = torch.linalg.eigvalsh(t_mats[i])\n",
    "            self.assertTrue(torch.allclose(eigs, approx_eigs, atol=1e-3, rtol=1e-4))\n",
    "\n",
    "    def test_batch_cg(self):\n",
    "        batch = 5\n",
    "        size = 100\n",
    "        matrix = torch.randn(batch, size, size, dtype=torch.float64)\n",
    "        matrix = matrix.matmul(matrix.mT)\n",
    "        matrix.div_(matrix.norm())\n",
    "        matrix.add_(torch.eye(matrix.size(-1), dtype=torch.float64).mul_(1e-1))\n",
    "\n",
    "        rhs = torch.randn(batch, size, 50, dtype=torch.float64)\n",
    "        solves = linear_cg(matrix.matmul, rhs=rhs, max_iter=size)\n",
    "\n",
    "        # Check cg\n",
    "        matrix_chol = torch.linalg.cholesky(matrix)\n",
    "        actual = torch.cholesky_solve(rhs, matrix_chol)\n",
    "        self.assertTrue(torch.allclose(solves, actual, atol=1e-3, rtol=1e-4))\n",
    "\n",
    "    def test_batch_cg_with_tridiag(self):\n",
    "        batch = 5\n",
    "        size = 10\n",
    "        matrix = torch.randn(batch, size, size, dtype=torch.float64)\n",
    "        matrix = matrix.matmul(matrix.mT)\n",
    "        matrix.div_(matrix.norm())\n",
    "        matrix.add_(torch.eye(matrix.size(-1), dtype=torch.float64).mul_(1e-1))\n",
    "\n",
    "        rhs = torch.randn(batch, size, 10, dtype=torch.float64)\n",
    "        with warnings.catch_warnings(record=True) as ws:\n",
    "            solves, t_mats = linear_cg(\n",
    "                matrix.matmul,\n",
    "                rhs=rhs,\n",
    "                n_tridiag=8,\n",
    "                max_iter=size,\n",
    "                max_tridiag_iter=10,\n",
    "                tolerance=0,\n",
    "                eps=1e-30,\n",
    "            )\n",
    "            self.assertTrue(any(issubclass(w.category, NumericalWarning) for w in ws))\n",
    "\n",
    "        # Check cg\n",
    "        matrix_chol = torch.linalg.cholesky(matrix)\n",
    "        actual = torch.cholesky_solve(rhs, matrix_chol)\n",
    "        self.assertTrue(torch.allclose(solves, actual, atol=1e-3, rtol=1e-4))\n",
    "\n",
    "        # Check tridiag\n",
    "        for i in range(5):\n",
    "            eigs = torch.linalg.eigvalsh(matrix[i])\n",
    "            for j in range(8):\n",
    "                approx_eigs = torch.linalg.eigvalsh(t_mats[j, i])\n",
    "                self.assertTrue(torch.allclose(eigs, approx_eigs, atol=1e-3, rtol=1e-4))\n",
    "\n",
    "    def test_batch_cg_init(self):\n",
    "        batch = 5\n",
    "        size = 100\n",
    "        matrix = torch.randn(batch, size, size, dtype=torch.float64)\n",
    "        matrix = matrix.matmul(matrix.mT)\n",
    "        matrix.div_(matrix.norm())\n",
    "        matrix.add_(torch.eye(matrix.size(-1), dtype=torch.float64).mul_(1e-1))\n",
    "\n",
    "        # Initial solve\n",
    "        rhs = torch.randn(batch, size, 50, dtype=torch.float64)\n",
    "        solves = linear_cg(matrix.matmul, rhs=rhs, max_iter=size, max_tridiag_iter=0)\n",
    "\n",
    "        # Initialize with solve\n",
    "        solves_with_init = linear_cg(matrix.matmul, rhs=rhs, max_iter=1, initial_guess=solves, max_tridiag_iter=0)\n",
    "\n",
    "        # Check cg\n",
    "        matrix_chol = torch.linalg.cholesky(matrix)\n",
    "        actual = torch.cholesky_solve(rhs, matrix_chol)\n",
    "        self.assertTrue(torch.allclose(solves_with_init, actual, atol=1e-3, rtol=1e-4))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=[\"\"], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...................................\n",
      "----------------------------------------------------------------------\n",
      "Ran 35 tests in 4.820s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import unittest\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "import torch\n",
    "\n",
    "import linear_operator\n",
    "from linear_operator.operators import DenseLinearOperator\n",
    "from linear_operator.test.base_test_case import BaseTestCase\n",
    "\n",
    "\n",
    "class TestInvQuadLogDetNonBatch(BaseTestCase, unittest.TestCase):\n",
    "    seed = 0\n",
    "    matrix_shape = torch.Size((50, 50))\n",
    "\n",
    "    def _test_inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, improper_logdet=False, add_diag=False):\n",
    "        # Set up\n",
    "        x = torch.randn(*self.__class__.matrix_shape[:-1], 3)\n",
    "        ls = torch.tensor(2.0).requires_grad_(True)\n",
    "        ls_clone = torch.tensor(2.0).requires_grad_(True)\n",
    "        mat = (x[..., :, None, :] - x[..., None, :, :]).pow(2.0).sum(dim=-1).mul(-0.5 * ls).exp()\n",
    "        mat_clone = (x[..., :, None, :] - x[..., None, :, :]).pow(2.0).sum(dim=-1).mul(-0.5 * ls_clone).exp()\n",
    "\n",
    "        if inv_quad_rhs is not None:\n",
    "            inv_quad_rhs.requires_grad_(True)\n",
    "            inv_quad_rhs_clone = inv_quad_rhs.detach().clone().requires_grad_(True)\n",
    "\n",
    "        mat_clone_with_diag = mat_clone\n",
    "        if add_diag:\n",
    "            mat_clone_with_diag = mat_clone_with_diag + torch.eye(mat_clone.size(-1))\n",
    "\n",
    "        if inv_quad_rhs is not None:\n",
    "            actual_inv_quad = mat_clone_with_diag.inverse().matmul(inv_quad_rhs_clone).mul(inv_quad_rhs_clone)\n",
    "            actual_inv_quad = actual_inv_quad.sum([-1, -2]) if inv_quad_rhs.dim() >= 2 else actual_inv_quad.sum()\n",
    "        if logdet:\n",
    "            flattened_tensor = mat_clone_with_diag.view(-1, *mat_clone.shape[-2:])\n",
    "            logdets = torch.cat([mat.logdet().unsqueeze(0) for mat in flattened_tensor])\n",
    "            if mat_clone.dim() > 2:\n",
    "                actual_logdet = logdets.view(*mat_clone.shape[:-2])\n",
    "            else:\n",
    "                actual_logdet = logdets.squeeze()\n",
    "\n",
    "        # Compute values with LinearOperator\n",
    "        _wrapped_cg = MagicMock(wraps=linear_operator.utils.linear_cg)\n",
    "        with linear_operator.settings.num_trace_samples(2000), linear_operator.settings.max_cholesky_size(\n",
    "            0\n",
    "        ), linear_operator.settings.cg_tolerance(1e-5), linear_operator.settings.skip_logdet_forward(\n",
    "            improper_logdet\n",
    "        ), patch(\n",
    "            \"linear_operator.utils.linear_cg\", new=_wrapped_cg\n",
    "        ) as linear_cg_mock, linear_operator.settings.min_preconditioning_size(\n",
    "            0\n",
    "        ), linear_operator.settings.max_preconditioner_size(\n",
    "            30\n",
    "        ):\n",
    "            linear_op = DenseLinearOperator(mat)\n",
    "\n",
    "            if add_diag:\n",
    "                linear_op = linear_op.add_jitter(1.0)\n",
    "\n",
    "            res_inv_quad, res_logdet = linear_operator.inv_quad_logdet(\n",
    "                linear_op, inv_quad_rhs=inv_quad_rhs, logdet=logdet\n",
    "            )\n",
    "\n",
    "        # Compare forward pass\n",
    "        if inv_quad_rhs is not None:\n",
    "            self.assertAllClose(res_inv_quad, actual_inv_quad, rtol=1e-2)\n",
    "        if logdet and not improper_logdet:\n",
    "            self.assertAllClose(res_logdet, actual_logdet, rtol=1e-1, atol=2e-1)\n",
    "\n",
    "        # Backward\n",
    "        if inv_quad_rhs is not None:\n",
    "            actual_inv_quad.sum().backward(retain_graph=True)\n",
    "            res_inv_quad.sum().backward(retain_graph=True)\n",
    "        if logdet:\n",
    "            actual_logdet.sum().backward()\n",
    "            res_logdet.sum().backward()\n",
    "\n",
    "        self.assertAllClose(ls.grad, ls_clone.grad, rtol=1e-2, atol=1e-2)\n",
    "        if inv_quad_rhs is not None:\n",
    "            self.assertAllClose(inv_quad_rhs.grad, inv_quad_rhs_clone.grad, rtol=2e-2, atol=1e-2)\n",
    "\n",
    "        # Make sure CG was called\n",
    "        self.assertTrue(linear_cg_mock.called)\n",
    "\n",
    "    def test_inv_quad_logdet_vector(self):\n",
    "        rhs = torch.randn(self.matrix_shape[-1])\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True)\n",
    "\n",
    "    def test_precond_inv_quad_logdet_vector(self):\n",
    "        rhs = torch.randn(self.matrix_shape[-1])\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True, add_diag=True)\n",
    "\n",
    "    def test_inv_quad_only_vector(self):\n",
    "        rhs = torch.randn(self.matrix_shape[-1])\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=False)\n",
    "\n",
    "    def test_precond_inv_quad_only_vector(self):\n",
    "        rhs = torch.randn(self.matrix_shape[-1])\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=False, add_diag=True)\n",
    "\n",
    "    def test_inv_quad_logdet_many_vectors(self):\n",
    "        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True)\n",
    "\n",
    "    def test_precond_inv_quad_logdet_many_vectors(self):\n",
    "        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True, add_diag=True)\n",
    "\n",
    "    def test_inv_quad_logdet_many_vectors_improper(self):\n",
    "        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True, improper_logdet=True)\n",
    "\n",
    "    def test_precond_inv_quad_logdet_many_vectors_improper(self):\n",
    "        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True, improper_logdet=True, add_diag=True)\n",
    "\n",
    "    def test_inv_quad_only_many_vectors(self):\n",
    "        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=False)\n",
    "\n",
    "    def test_precond_inv_quad_only_many_vectors(self):\n",
    "        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=False, add_diag=True)\n",
    "\n",
    "\n",
    "class TestInvQuadLogDetBatch(TestInvQuadLogDetNonBatch):\n",
    "    seed = 0\n",
    "    matrix_shape = torch.Size((3, 50, 50))\n",
    "\n",
    "    def test_inv_quad_logdet_vector(self):\n",
    "        pass\n",
    "\n",
    "    def test_precond_inv_quad_logdet_vector(self):\n",
    "        pass\n",
    "\n",
    "    def test_inv_quad_only_vector(self):\n",
    "        pass\n",
    "\n",
    "    def test_precond_inv_quad_only_vector(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TestInvQuadLogDetMultiBatch(TestInvQuadLogDetBatch):\n",
    "    seed = 0\n",
    "    matrix_shape = torch.Size((2, 3, 50, 50))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=[\"\"], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....................................\n",
      "----------------------------------------------------------------------\n",
      "Ran 37 tests in 4.083s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import unittest\n",
    "\n",
    "import torch\n",
    "\n",
    "import linear_operator\n",
    "from linear_operator.test.base_test_case import BaseTestCase\n",
    "from linear_operator.utils.cholesky import psd_safe_cholesky\n",
    "from linear_operator.utils.permutation import apply_permutation, inverse_permutation\n",
    "\n",
    "\n",
    "def _ensure_symmetric_grad(grad):\n",
    "    \"\"\"\n",
    "    A gradient-hook hack to ensure that symmetric matrix gradients are symmetric\n",
    "    \"\"\"\n",
    "    res = torch.add(grad, grad.mT).mul(0.5)\n",
    "    return res\n",
    "\n",
    "\n",
    "class TestPivotedCholesky(BaseTestCase, unittest.TestCase):\n",
    "    seed = 0\n",
    "\n",
    "    def _create_mat(self):\n",
    "        mat = torch.randn(8, 8)\n",
    "        mat = mat @ mat.mT\n",
    "        return mat\n",
    "\n",
    "    def test_pivoted_cholesky(self, max_iter=3):\n",
    "        mat = self._create_mat().detach().requires_grad_(True)\n",
    "        mat.register_hook(_ensure_symmetric_grad)\n",
    "        mat_copy = mat.detach().clone().requires_grad_(True)\n",
    "        mat_copy.register_hook(_ensure_symmetric_grad)\n",
    "\n",
    "        # Forward (with function)\n",
    "        res, pivots = linear_operator.pivoted_cholesky(mat, rank=max_iter, return_pivots=True)\n",
    "\n",
    "        # Forward (manual pivoting, actual Cholesky)\n",
    "        inverse_pivots = inverse_permutation(pivots)\n",
    "        # Apply pivoting\n",
    "        pivoted_mat_copy = apply_permutation(mat_copy, pivots, pivots)\n",
    "        # Compute Cholesky\n",
    "        actual_pivoted = psd_safe_cholesky(pivoted_mat_copy)[..., :max_iter]\n",
    "        # Undo pivoting\n",
    "        actual = apply_permutation(actual_pivoted, left_permutation=inverse_pivots)\n",
    "\n",
    "        self.assertAllClose(res, actual)\n",
    "\n",
    "        # Backward\n",
    "        grad_output = torch.randn_like(res)\n",
    "        res.backward(gradient=grad_output)\n",
    "        actual.backward(gradient=grad_output)\n",
    "        self.assertAllClose(mat.grad, mat_copy.grad)\n",
    "\n",
    "\n",
    "class TestPivotedCholeskyBatch(TestPivotedCholesky, unittest.TestCase):\n",
    "    seed = 0\n",
    "\n",
    "    def _create_mat(self):\n",
    "        mat = torch.randn(2, 3, 8, 8)\n",
    "        mat = mat @ mat.mT\n",
    "        return mat\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=[\"\"], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# linear_operator packages\n",
    "from linear_operator import inv_quad_logdet\n",
    "from linear_operator.operators import DenseLinearOperator\n",
    "\n",
    "class IterativeGP(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple illustration of using linear_operator's inv_quad_logdet\n",
    "    to fit a GP model given a kernel matrix + diagonal noise.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel, noise=0.1):\n",
    "        super().__init__()\n",
    "        # \"kernel\" can be something from GPyTorch (e.g. RBFKernel),\n",
    "        # or any callable that can produce an (n x n) covariance matrix\n",
    "        self.kernel = kernel\n",
    "        # We make noise a Parameter so that it can be trained via gradient\n",
    "        self.raw_noise = nn.Parameter(torch.log(torch.tensor(noise, dtype=torch.float32)))\n",
    "\n",
    "    def noise_value(self):\n",
    "        # Use softplus or exponent to keep noise strictly positive\n",
    "        return torch.exp(self.raw_noise)\n",
    "\n",
    "    def fit(self, train_x, train_y):\n",
    "        \"\"\"\n",
    "        train_x: tensor of shape (n x d)\n",
    "        train_y: tensor of shape (n,) or (n x 1)\n",
    "\n",
    "        Returns: The scalar MLL (float) after computing inv_quad and logdet.\n",
    "        \"\"\"\n",
    "        train_x = train_x.detach().clone()\n",
    "        train_y = train_y.detach().clone()\n",
    "\n",
    "        # Mark them as requiring gradient only if you want to backprop\n",
    "        # through train_x or train_y themselves (usually you do not)\n",
    "        # train_x.requires_grad_(False)\n",
    "        # train_y.requires_grad_(False)\n",
    "\n",
    "        # Produce a dense covariance matrix from the kernel\n",
    "        K = self.kernel(train_x, train_x).evaluate()  # shape (n, n)\n",
    "        # Add noise on the diagonal\n",
    "        K = K + self.noise_value() * torch.eye(K.size(-1), dtype=K.dtype, device=K.device)\n",
    "\n",
    "        # Wrap the matrix with DenseLinearOperator\n",
    "        linear_op = DenseLinearOperator(K)\n",
    "\n",
    "        # If train_y is just shape (n,), pass it as inv_quad_rhs\n",
    "        # logdet=True means we also want the approximate log|K|\n",
    "        inv_quad_term, logdet_term = inv_quad_logdet(\n",
    "            linear_op,\n",
    "            inv_quad_rhs=train_y,\n",
    "            logdet=True\n",
    "        )\n",
    "\n",
    "        n = train_y.size(0)\n",
    "        # Standard Gaussian log-likelihood:\n",
    "        # 0.5 * ( y^T K^{-1} y + log|K| + n * log(2π) )\n",
    "        const = n * torch.log(torch.tensor(2.0 * 3.141592653589793, dtype=K.dtype, device=K.device))\n",
    "        mll = 0.5 * (inv_quad_term + logdet_term + const)\n",
    "\n",
    "        # Backprop:\n",
    "        mll.backward()  # This will compute d/d(kernel params) and d/d(noise)\n",
    "        \n",
    "        # Return the numeric value\n",
    "        return mll.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : \n",
      "    The default C++ compiler could not be found on your system.\n",
      "    You need to either define the CXX environment variable or a symlink to the g++ command.\n",
      "    For example if g++-8 is the command you can do\n",
      "      import os\n",
      "      os.environ['CXX'] = 'g++-8'\n",
      "    \n",
      "[KeOps] Warning : Cuda libraries were not detected on the system or could not be loaded ; using cpu only mode\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "from gpytorch.kernels import RBFKernel,MaternKernel,ScaleKernel\n",
    "from gpytorch.priors import GammaPrior\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "notebook_dir = os.getcwd()\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '../code'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "from gps import CholeskyGaussianProcess\n",
    "from plotting import plot_gp_simple,plot_gp_sample\n",
    "from util import train,eval,plot_gpr_results\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# np.random.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "device=\"cuda:0\"\n",
    "global_dtype=torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data generation function with an abstract true function\n",
    "def generate_data(true_function, train_range=(-3, 3), test_range=(-3, 3), \n",
    "                  n_train=40, n_test=100, noise_std=0.1, \n",
    "                  device='cuda:0', dtype=torch.float64):\n",
    "    # Generate training data\n",
    "    X_train = torch.linspace(train_range[0], train_range[1], n_train, dtype=dtype, device=device).unsqueeze(-1)\n",
    "    y_train = true_function(X_train) + noise_std * torch.randn_like(X_train)\n",
    "    \n",
    "    # Generate test data\n",
    "    X_test = torch.linspace(test_range[0], test_range[1], n_test, dtype=dtype, device=device).unsqueeze(-1)\n",
    "    y_test = true_function(X_test)  # No noise added to test data\n",
    "    \n",
    "    return X_train, y_train.squeeze(), X_test, y_test.squeeze()\n",
    "\n",
    "# Define the true function\n",
    "def true_function(x):\n",
    "    \n",
    "    return torch.sin(2 * x) + torch.cos(3 * x)\n",
    "\n",
    "# Generate data using the true function\n",
    "train_x, train_y, test_x, test_y = generate_data(true_function,train_range=(-3, 3), test_range=(-5,5), dtype=global_dtype)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from linear_operator.operators import DenseLinearOperator\n",
    "from linear_operator import inv_quad_logdet\n",
    "\n",
    "\n",
    "class IterativeGP(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal iterative Gaussian Process model that:\n",
    "      - Does a single pass of fit(...) with inv_quad_logdet\n",
    "      - Caches logdet(K) and inv_quad term for repeated usage\n",
    "      - Provides a separate compute_mll(...) method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel, noise=0.1, dtype=torch.float64, device=\"cuda:0\"):\n",
    "        super().__init__()\n",
    "        self.kernel = kernel\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        # Use a raw noise parameter if you want to backprop through noise\n",
    "        self.raw_noise = nn.Parameter(torch.log(torch.tensor(noise, dtype=self.dtype, device=self.device)))\n",
    "\n",
    "        # We'll cache these during fit\n",
    "        self.cached_logdet = None\n",
    "        self.cached_inv_quad = None\n",
    "        self.cached_mll = None\n",
    "\n",
    "    def noise_value(self):\n",
    "        \"\"\"\n",
    "        Exponentiate raw_noise to keep it strictly > 0\n",
    "        \"\"\"\n",
    "        return torch.exp(self.raw_noise)\n",
    "\n",
    "    def fit(self, train_x, train_y):\n",
    "        \"\"\"\n",
    "        Perform a single 'fit' step:\n",
    "         1) Build K = kernel(...) + noise * I\n",
    "         2) inv_quad_logdet(...) => y^T K^{-1} y, logdet(K)\n",
    "         3) Store/copy to self.cached_(...) variables\n",
    "         4) (Optional) call backward if you want to do gradient-based updates right away\n",
    "        \"\"\"\n",
    "        train_x = train_x.to(self.device, self.dtype)\n",
    "        train_y = train_y.to(self.device, self.dtype)\n",
    "\n",
    "        # Build kernel matrix\n",
    "        K = self.kernel(train_x, train_x).evaluate()  # (n x n)\n",
    "        K = K + self.noise_value() * torch.eye(\n",
    "            K.size(0), dtype=self.dtype, device=self.device\n",
    "        )\n",
    "\n",
    "        # Wrap in DenseLinearOperator\n",
    "        lin_op = DenseLinearOperator(K)\n",
    "\n",
    "        # Compute inverse-quad & logdet\n",
    "        # inv_quad_rhs=train_y => y^T K^{-1} y\n",
    "        # logdet=True => approximate log|K|\n",
    "        inv_quad_term, logdet_term = inv_quad_logdet(\n",
    "            lin_op, inv_quad_rhs=train_y, logdet=True\n",
    "        )\n",
    "\n",
    "        # Optionally store for repeated usage\n",
    "        self.cached_inv_quad = inv_quad_term.detach()\n",
    "        self.cached_logdet = logdet_term.detach()\n",
    "\n",
    "        # Also store the final MLL if you like\n",
    "        n = train_y.size(0)\n",
    "        const = n * torch.log(torch.tensor(2.0 * 3.141592653589793, dtype=self.dtype, device=self.device))\n",
    "        mll = 0.5 * (inv_quad_term + logdet_term + const)\n",
    "\n",
    "        self.cached_mll = mll.detach()\n",
    "\n",
    "        # If you want to do a single backward pass right now, uncomment:\n",
    "        # mll.backward()\n",
    "\n",
    "    def compute_mll(self):\n",
    "        \"\"\"\n",
    "        Given that fit(...) has cached inv_quad and logdet,\n",
    "        compute the same MLL without re-running inv_quad_logdet.\n",
    "        \"\"\"\n",
    "        if self.cached_inv_quad is None or self.cached_logdet is None:\n",
    "            raise RuntimeError(\"Must call fit(...) first to cache inv_quad/logdet values.\")\n",
    "\n",
    "        # We'll need to know the size of training data for the constant term\n",
    "        # but we haven't stored train_y. Let's assume we also stored n in fit:\n",
    "        n = self.cached_inv_quad.size(0) if self.cached_inv_quad.dim() > 0 else 1\n",
    "        # Actually, if y was shape (n,), then inv_quad is just a scalar,\n",
    "        # so we can't parse n from that. Instead, suppose we stored `self.n_train` in fit(...).\n",
    "\n",
    "        # If we have self.cached_mll from fit, we can just return that:\n",
    "        if self.cached_mll is not None:\n",
    "            return self.cached_mll\n",
    "\n",
    "        # Or recompute from cached scalars:\n",
    "        #  mll = 0.5 * ( self.cached_inv_quad + self.cached_logdet + n*log(2π) )\n",
    "        #  return mll\n",
    "\n",
    "        raise NotImplementedError(\"Either store self.n_train or store self.cached_mll in fit(...) and just return it.\")\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        \"\"\"\n",
    "        Optional. If you want to do posterior prediction with CG solves:\n",
    "         - Build (K + noise*I) again\n",
    "         - Solve for alpha = K^{-1} y\n",
    "         - Then compute K(X_star, X) alpha for the predictive mean\n",
    "         - Possibly multiple solves for the predictive covariance\n",
    "        We'll keep it minimal and leave it as a placeholder.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement iterative solves for predictive mean/cov if desired.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "from linear_operator import settings\n",
    "from linear_operator.utils.lanczos import lanczos_tridiag_to_diag\n",
    "from linear_operator.utils.stochastic_lq import StochasticLQ\n",
    "\n",
    "\n",
    "class customInvQuadLogdet(Function):\n",
    "    \"\"\"\n",
    "    Given a PSD matrix A (or a batch of PSD matrices A), this function computes one or both\n",
    "    of the following\n",
    "    - The matrix solves A^{-1} b\n",
    "    - logdet(A)\n",
    "\n",
    "    This function uses preconditioned CG and Lanczos quadrature to compute the inverse quadratic\n",
    "    and log determinant terms, using the variance reduction strategy outlined in:\n",
    "    ``Reducing the Variance of Gaussian Process Hyperparameter Optimization with Preconditioning''\n",
    "    (https://arxiv.org/abs/2107.00243)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx,\n",
    "        representation_tree,\n",
    "        precond_representation_tree,\n",
    "        preconditioner,\n",
    "        num_precond_args,\n",
    "        inv_quad,\n",
    "        probe_vectors,\n",
    "        probe_vector_norms,\n",
    "        *args,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        *args - The arguments representing the PSD matrix A (or batch of PSD matrices A)\n",
    "        If self.inv_quad is true, the first entry in *args is inv_quad_rhs (Tensor)\n",
    "        - the RHS of the matrix solves.\n",
    "\n",
    "        Returns:\n",
    "        - (Scalar) The inverse quadratic form (or None, if self.inv_quad is False)\n",
    "        - (Scalar) The log determinant (or None, self.if logdet is False)\n",
    "        \"\"\"\n",
    "\n",
    "        ctx.representation_tree = representation_tree\n",
    "        ctx.precond_representation_tree = precond_representation_tree\n",
    "        ctx.preconditioner = preconditioner\n",
    "        ctx.inv_quad = inv_quad\n",
    "        ctx.num_precond_args = num_precond_args\n",
    "\n",
    "        matrix_args = None\n",
    "        precond_args = tuple()\n",
    "        inv_quad_rhs = None\n",
    "        if ctx.inv_quad:\n",
    "            inv_quad_rhs = args[0]\n",
    "            args = args[1:]\n",
    "        if ctx.num_precond_args:\n",
    "            matrix_args = args[:-num_precond_args]\n",
    "            precond_args = args[-num_precond_args:]\n",
    "        else:\n",
    "            matrix_args = args\n",
    "\n",
    "        # Get closure for matmul\n",
    "        linear_op = ctx.representation_tree(*matrix_args)\n",
    "        precond_lt = ctx.precond_representation_tree(*precond_args)\n",
    "\n",
    "        # Get info about matrix\n",
    "        ctx.dtype = linear_op.dtype\n",
    "        ctx.device = linear_op.device\n",
    "        ctx.matrix_shape = linear_op.matrix_shape\n",
    "        ctx.batch_shape = linear_op.batch_shape\n",
    "\n",
    "        # Probe vectors\n",
    "        if probe_vectors is None or probe_vector_norms is None:\n",
    "            num_random_probes = settings.num_trace_samples.value()\n",
    "            if settings.deterministic_probes.on():\n",
    "                # NOTE: calling precond_lt.root_decomposition() is expensive\n",
    "                # because it requires Lanczos\n",
    "                # We don't have any other choice for when we want to use deterministic probes, however\n",
    "                if precond_lt.size()[-2:] == torch.Size([1, 1]):\n",
    "                    covar_root = precond_lt.to_dense().sqrt()\n",
    "                else:\n",
    "                    covar_root = precond_lt.root_decomposition().root\n",
    "\n",
    "                warnings.warn(\n",
    "                    \"The deterministic probes feature is now deprecated. \"\n",
    "                    \"See https://github.com/cornellius-gp/linear_operator/pull/1836.\",\n",
    "                    DeprecationWarning,\n",
    "                )\n",
    "                base_samples = settings.deterministic_probes.probe_vectors\n",
    "                if base_samples is None or covar_root.size(-1) != base_samples.size(-2):\n",
    "                    base_samples = torch.randn(\n",
    "                        *precond_lt.batch_shape,\n",
    "                        covar_root.size(-1),\n",
    "                        num_random_probes,\n",
    "                        dtype=precond_lt.dtype,\n",
    "                        device=precond_lt.device,\n",
    "                    )\n",
    "                    settings.deterministic_probes.probe_vectors = base_samples\n",
    "\n",
    "                probe_vectors = covar_root.matmul(base_samples).permute(-1, *range(precond_lt.dim() - 1))\n",
    "            else:\n",
    "                probe_vectors = precond_lt.zero_mean_mvn_samples(num_random_probes)\n",
    "            probe_vectors = probe_vectors.unsqueeze(-2).transpose(0, -2).squeeze(0).mT.contiguous()\n",
    "            probe_vector_norms = torch.norm(probe_vectors, p=2, dim=-2, keepdim=True)\n",
    "            probe_vectors = probe_vectors.div(probe_vector_norms)\n",
    "\n",
    "        # Probe vectors\n",
    "        ctx.probe_vectors = probe_vectors\n",
    "        ctx.probe_vector_norms = probe_vector_norms\n",
    "\n",
    "        # Collect terms for LinearCG\n",
    "        # We use LinearCG for both matrix solves and for stochastically estimating the log det\n",
    "        rhs_list = [ctx.probe_vectors]\n",
    "        num_random_probes = ctx.probe_vectors.size(-1)\n",
    "        num_inv_quad_solves = 0\n",
    "\n",
    "        # RHS for inv_quad\n",
    "        ctx.is_vector = False\n",
    "        if ctx.inv_quad:\n",
    "            if inv_quad_rhs.ndimension() == 1:\n",
    "                inv_quad_rhs = inv_quad_rhs.unsqueeze(-1)\n",
    "                ctx.is_vector = True\n",
    "            rhs_list.append(inv_quad_rhs)\n",
    "            num_inv_quad_solves = inv_quad_rhs.size(-1)\n",
    "\n",
    "        # Perform solves (for inv_quad) and tridiagonalization (for estimating logdet)\n",
    "        rhs = torch.cat(rhs_list, -1)\n",
    "        solves, t_mat = linear_op._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
    "\n",
    "        # Final values to return\n",
    "        logdet_term = torch.zeros(linear_op.batch_shape, dtype=ctx.dtype, device=ctx.device)\n",
    "        inv_quad_term = torch.zeros(linear_op.batch_shape, dtype=ctx.dtype, device=ctx.device)\n",
    "\n",
    "        # Compute logdet from tridiagonalization\n",
    "        if settings.skip_logdet_forward.off():\n",
    "            if torch.any(torch.isnan(t_mat)).item():\n",
    "                logdet_term = torch.tensor(float(\"nan\"), dtype=ctx.dtype, device=ctx.device)\n",
    "            else:\n",
    "                if ctx.batch_shape is None:\n",
    "                    t_mat = t_mat.unsqueeze(1)\n",
    "                eigenvalues, eigenvectors = lanczos_tridiag_to_diag(t_mat)\n",
    "                slq = StochasticLQ()\n",
    "                (logdet_term,) = slq.to_dense(ctx.matrix_shape, eigenvalues, eigenvectors, [lambda x: x.log()])\n",
    "\n",
    "        # Extract inv_quad solves from all the solves\n",
    "        if ctx.inv_quad:\n",
    "            inv_quad_solves = solves.narrow(-1, num_random_probes, num_inv_quad_solves)\n",
    "            inv_quad_term = (inv_quad_solves * inv_quad_rhs).sum(-2)\n",
    "\n",
    "        ctx.num_random_probes = num_random_probes\n",
    "        ctx.num_inv_quad_solves = num_inv_quad_solves\n",
    "\n",
    "        to_save = list(precond_args) + list(matrix_args) + [solves]\n",
    "        ctx.save_for_backward(*to_save)\n",
    "\n",
    "        return inv_quad_term, logdet_term\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, inv_quad_grad_output, logdet_grad_output):\n",
    "        # Get input arguments, and get gradients in the proper form\n",
    "        if ctx.num_precond_args:\n",
    "            precond_args = ctx.saved_tensors[: ctx.num_precond_args]\n",
    "            matrix_args = ctx.saved_tensors[ctx.num_precond_args : -1]\n",
    "        else:\n",
    "            precond_args = []\n",
    "            matrix_args = ctx.saved_tensors[:-1]\n",
    "        solves = ctx.saved_tensors[-1]\n",
    "\n",
    "        linear_op = ctx.representation_tree(*matrix_args)\n",
    "        precond_lt = ctx.precond_representation_tree(*precond_args)\n",
    "\n",
    "        # Fix grad_output sizes\n",
    "        if ctx.inv_quad:\n",
    "            inv_quad_grad_output = inv_quad_grad_output.unsqueeze(-2)\n",
    "        logdet_grad_output = logdet_grad_output.unsqueeze(-1)\n",
    "        logdet_grad_output.unsqueeze_(-1)\n",
    "\n",
    "        # Un-normalize probe vector solves\n",
    "        coef = 1.0 / ctx.probe_vectors.size(-1)\n",
    "        probe_vector_solves = solves.narrow(-1, 0, ctx.num_random_probes).mul(coef)\n",
    "        probe_vector_solves.mul_(ctx.probe_vector_norms).mul_(logdet_grad_output)\n",
    "\n",
    "        # Apply preconditioner to probe vectors (originally drawn from N(0, P))\n",
    "        # Now the probe vectors will be drawn from N(0, P^{-1})\n",
    "        if ctx.preconditioner is not None:\n",
    "            precond_probe_vectors = ctx.preconditioner(ctx.probe_vectors * ctx.probe_vector_norms)\n",
    "        else:\n",
    "            precond_probe_vectors = ctx.probe_vectors * ctx.probe_vector_norms\n",
    "\n",
    "        # matrix gradient\n",
    "        # Collect terms for arg grads\n",
    "        left_factors_list = [probe_vector_solves]\n",
    "        right_factors_list = [precond_probe_vectors]\n",
    "\n",
    "        inv_quad_solves = None\n",
    "        neg_inv_quad_solves_times_grad_out = None\n",
    "        if ctx.inv_quad:\n",
    "            inv_quad_solves = solves.narrow(-1, ctx.num_random_probes, ctx.num_inv_quad_solves)\n",
    "            neg_inv_quad_solves_times_grad_out = inv_quad_solves.mul(inv_quad_grad_output).mul_(-1)\n",
    "            left_factors_list.append(neg_inv_quad_solves_times_grad_out)\n",
    "            right_factors_list.append(inv_quad_solves)\n",
    "\n",
    "        left_factors = torch.cat(left_factors_list, -1)\n",
    "        right_factors = torch.cat(right_factors_list, -1)\n",
    "        matrix_arg_grads = linear_op._bilinear_derivative(left_factors, right_factors)\n",
    "\n",
    "        # precond gradient\n",
    "        precond_arg_grads = precond_lt._bilinear_derivative(\n",
    "            -precond_probe_vectors * coef, precond_probe_vectors * logdet_grad_output\n",
    "        )\n",
    "\n",
    "        # inv_quad_rhs gradients\n",
    "        if ctx.inv_quad:\n",
    "            inv_quad_rhs_grad = neg_inv_quad_solves_times_grad_out.mul_(-2)\n",
    "            if ctx.is_vector:\n",
    "                inv_quad_rhs_grad.squeeze_(-1)\n",
    "            res = [inv_quad_rhs_grad] + list(matrix_arg_grads) + list(precond_arg_grads)\n",
    "        else:\n",
    "            res = list(matrix_arg_grads) + list(precond_arg_grads)\n",
    "\n",
    "        return tuple([None] * 7 + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fredw\\AppData\\Local\\Temp\\ipykernel_10904\\2642748399.py:38: DeprecationWarning: <class '__main__.customInvQuadLogdet'> should not be instantiated. Methods on autograd functionsare all static, so you should invoke them on the class itself. Instantiating an autograd function will raise an error in a future version of PyTorch.\n",
      "  inv_quad_term, logdet_term = customInvQuadLogdet(self.K, inv_quad_rhs=self.train_y.unsqueeze(-1), logdet=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable customInvQuadLogdet object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m kernel \u001b[38;5;241m=\u001b[39m ScaleKernel(base_kernel, outputscale_prior\u001b[38;5;241m=\u001b[39mGammaPrior(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3.0\u001b[39m, \u001b[38;5;241m3.0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     51\u001b[0m igp1 \u001b[38;5;241m=\u001b[39m IterativeGP2(kernel, noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 52\u001b[0m \u001b[43migp1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m mll_value \u001b[38;5;241m=\u001b[39m igp\u001b[38;5;241m.\u001b[39mcompute_mll()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(mll_value\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[1;32mIn[5], line 38\u001b[0m, in \u001b[0;36mIterativeGP2.fit\u001b[1;34m(self, train_x, train_y)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK\u001b[38;5;241m.\u001b[39madd_diag(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_value()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# add_jitter(self.noise_value())\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m inv_quad_term, logdet_term \u001b[38;5;241m=\u001b[39m customInvQuadLogdet(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK, inv_quad_rhs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), logdet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogdet_term\u001b[38;5;241m=\u001b[39m logdet_term\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minv_quad_term \u001b[38;5;241m=\u001b[39m inv_quad_term\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable customInvQuadLogdet object"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.priors import GammaPrior\n",
    "from linear_operator import inv_quad_logdet\n",
    "\n",
    "def generate_data(f, train_range=(-3, 3), test_range=(-3, 3), n_train=40, n_test=100, noise_std=0.1, device='cuda:0', dtype=torch.float64):\n",
    "    X_train = torch.linspace(train_range[0], train_range[1], n_train, dtype=dtype, device=device).unsqueeze(-1)\n",
    "    y_train = f(X_train) + noise_std * torch.randn_like(X_train)\n",
    "    X_test = torch.linspace(test_range[0], test_range[1], n_test, dtype=dtype, device=device).unsqueeze(-1)\n",
    "    y_test = f(X_test)\n",
    "    return X_train, y_train.squeeze(), X_test, y_test.squeeze()\n",
    "\n",
    "def true_function(x):\n",
    "    return torch.sin(2 * x) + torch.cos(3 * x)\n",
    "\n",
    "class IterativeGP2(nn.Module):\n",
    "    def __init__(self, kernel, noise=0.1, dtype=torch.float64, device=\"cuda:0\"):\n",
    "        super().__init__()\n",
    "        self.kernel = kernel\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.raw_noise = nn.Parameter(torch.log(torch.tensor(noise, dtype=self.dtype, device=self.device)))\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.logdet_term = None\n",
    "\n",
    "    def noise_value(self):\n",
    "        return torch.exp(self.raw_noise)\n",
    "\n",
    "    def fit(self, train_x, train_y):\n",
    "        self.train_x = train_x.to(self.device, self.dtype)\n",
    "        self.train_y = train_y.to(self.device, self.dtype)\n",
    "\n",
    "        self.K = self.kernel(self.train_x, self.train_x)\n",
    "        self.K.add_diag(self.noise_value()**2)\n",
    "        # add_jitter(self.noise_value())\n",
    "        inv_quad_term, logdet_term = customInvQuadLogdet(self.K, inv_quad_rhs=self.train_y.unsqueeze(-1), logdet=True)\n",
    "        self.logdet_term= logdet_term\n",
    "        self.inv_quad_term = inv_quad_term\n",
    "\n",
    "    def compute_mll(self):\n",
    "        n = self.train_y.size(0)\n",
    "        const = n * torch.log(torch.tensor(2.0 * 3.141592653589793, dtype=self.dtype, device=self.device))\n",
    "        return 0.5 * (self.inv_quad_term + self.logdet_term + const)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_x, train_y, test_x, test_y = generate_data(true_function, train_range=(-3,3), test_range=(-5,5), device=device)\n",
    "base_kernel = MaternKernel(ard_num_dims=train_x.shape[-1], nu=1.5, lengthscale_prior=GammaPrior(-3.0, 3.0))\n",
    "kernel = ScaleKernel(base_kernel, outputscale_prior=GammaPrior(-3.0, 3.0)).to(device)\n",
    "igp1 = IterativeGP2(kernel, noise=0.4, dtype=torch.float64, device=device)\n",
    "igp1.fit(train_x, train_y)\n",
    "mll_value = igp.compute_mll()\n",
    "print(mll_value.item())\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_x, train_y, test_x, test_y = generate_data(true_function, train_range=(-3,3), test_range=(-5,5), device=device)\n",
    "base_kernel = MaternKernel(ard_num_dims=train_x.shape[-1], nu=1.5, lengthscale_prior=GammaPrior(-3.0, 3.0))\n",
    "kernel = ScaleKernel(base_kernel, outputscale_prior=GammaPrior(-3.0, 3.0)).to(device)\n",
    "igp2 = IterativeGP(kernel, noise=0.4, dtype=torch.float64, device=device)\n",
    "igp2.fit(train_x, train_y)\n",
    "mll_value = igp.compute_mll()\n",
    "print(mll_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
