{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "----------------------------------------------------------------------\n",
      "Ran 30 tests in 4.230s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import unittest\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "import torch\n",
    "\n",
    "import linear_operator\n",
    "from linear_operator.operators import DenseLinearOperator\n",
    "from linear_operator.test.base_test_case import BaseTestCase\n",
    "\n",
    "\n",
    "class TestInvQuadLogDetNonBatch(BaseTestCase, unittest.TestCase):\n",
    "    seed = 0\n",
    "    matrix_shape = torch.Size((50, 50))\n",
    "\n",
    "    def _test_inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, improper_logdet=False, add_diag=False):\n",
    "        # Set up\n",
    "        x = torch.randn(*self.__class__.matrix_shape[:-1], 3)\n",
    "        ls = torch.tensor(2.0).requires_grad_(True)\n",
    "        ls_clone = torch.tensor(2.0).requires_grad_(True)\n",
    "        mat = (x[..., :, None, :] - x[..., None, :, :]).pow(2.0).sum(dim=-1).mul(-0.5 * ls).exp()\n",
    "        mat_clone = (x[..., :, None, :] - x[..., None, :, :]).pow(2.0).sum(dim=-1).mul(-0.5 * ls_clone).exp()\n",
    "\n",
    "        if inv_quad_rhs is not None:\n",
    "            inv_quad_rhs.requires_grad_(True)\n",
    "            inv_quad_rhs_clone = inv_quad_rhs.detach().clone().requires_grad_(True)\n",
    "\n",
    "        mat_clone_with_diag = mat_clone\n",
    "        if add_diag:\n",
    "            mat_clone_with_diag = mat_clone_with_diag + torch.eye(mat_clone.size(-1))\n",
    "\n",
    "        if inv_quad_rhs is not None:\n",
    "            actual_inv_quad = mat_clone_with_diag.inverse().matmul(inv_quad_rhs_clone).mul(inv_quad_rhs_clone)\n",
    "            actual_inv_quad = actual_inv_quad.sum([-1, -2]) if inv_quad_rhs.dim() >= 2 else actual_inv_quad.sum()\n",
    "        if logdet:\n",
    "            flattened_tensor = mat_clone_with_diag.view(-1, *mat_clone.shape[-2:])\n",
    "            logdets = torch.cat([mat.logdet().unsqueeze(0) for mat in flattened_tensor])\n",
    "            if mat_clone.dim() > 2:\n",
    "                actual_logdet = logdets.view(*mat_clone.shape[:-2])\n",
    "            else:\n",
    "                actual_logdet = logdets.squeeze()\n",
    "\n",
    "        # Compute values with LinearOperator\n",
    "        _wrapped_cg = MagicMock(wraps=linear_operator.utils.linear_cg)\n",
    "        with linear_operator.settings.num_trace_samples(2000), linear_operator.settings.max_cholesky_size(\n",
    "            0\n",
    "        ), linear_operator.settings.cg_tolerance(1e-5), linear_operator.settings.skip_logdet_forward(\n",
    "            improper_logdet\n",
    "        ), patch(\n",
    "            \"linear_operator.utils.linear_cg\", new=_wrapped_cg\n",
    "        ) as linear_cg_mock, linear_operator.settings.min_preconditioning_size(\n",
    "            0\n",
    "        ), linear_operator.settings.max_preconditioner_size(\n",
    "            30\n",
    "        ):\n",
    "            linear_op = DenseLinearOperator(mat)\n",
    "\n",
    "            if add_diag:\n",
    "                linear_op = linear_op.add_jitter(1.0)\n",
    "\n",
    "            res_inv_quad, res_logdet = linear_operator.inv_quad_logdet(\n",
    "                linear_op, inv_quad_rhs=inv_quad_rhs, logdet=logdet\n",
    "            )\n",
    "\n",
    "        # Compare forward pass\n",
    "        if inv_quad_rhs is not None:\n",
    "            self.assertAllClose(res_inv_quad, actual_inv_quad, rtol=1e-2)\n",
    "        if logdet and not improper_logdet:\n",
    "            self.assertAllClose(res_logdet, actual_logdet, rtol=1e-1, atol=2e-1)\n",
    "\n",
    "        # Backward\n",
    "        if inv_quad_rhs is not None:\n",
    "            actual_inv_quad.sum().backward(retain_graph=True)\n",
    "            res_inv_quad.sum().backward(retain_graph=True)\n",
    "        if logdet:\n",
    "            actual_logdet.sum().backward()\n",
    "            res_logdet.sum().backward()\n",
    "\n",
    "        self.assertAllClose(ls.grad, ls_clone.grad, rtol=1e-2, atol=1e-2)\n",
    "        if inv_quad_rhs is not None:\n",
    "            self.assertAllClose(inv_quad_rhs.grad, inv_quad_rhs_clone.grad, rtol=2e-2, atol=1e-2)\n",
    "\n",
    "        # Make sure CG was called\n",
    "        self.assertTrue(linear_cg_mock.called)\n",
    "\n",
    "    def test_inv_quad_logdet_vector(self):\n",
    "        rhs = torch.randn(self.matrix_shape[-1])\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True)\n",
    "\n",
    "    def test_precond_inv_quad_logdet_vector(self):\n",
    "        rhs = torch.randn(self.matrix_shape[-1])\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True, add_diag=True)\n",
    "\n",
    "    def test_inv_quad_only_vector(self):\n",
    "        rhs = torch.randn(self.matrix_shape[-1])\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=False)\n",
    "\n",
    "    def test_precond_inv_quad_only_vector(self):\n",
    "        rhs = torch.randn(self.matrix_shape[-1])\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=False, add_diag=True)\n",
    "\n",
    "    def test_inv_quad_logdet_many_vectors(self):\n",
    "        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True)\n",
    "\n",
    "    def test_precond_inv_quad_logdet_many_vectors(self):\n",
    "        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True, add_diag=True)\n",
    "\n",
    "    def test_inv_quad_logdet_many_vectors_improper(self):\n",
    "        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True, improper_logdet=True)\n",
    "\n",
    "    def test_precond_inv_quad_logdet_many_vectors_improper(self):\n",
    "        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True, improper_logdet=True, add_diag=True)\n",
    "\n",
    "    def test_inv_quad_only_many_vectors(self):\n",
    "        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=False)\n",
    "\n",
    "    def test_precond_inv_quad_only_many_vectors(self):\n",
    "        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n",
    "        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=False, add_diag=True)\n",
    "\n",
    "\n",
    "class TestInvQuadLogDetBatch(TestInvQuadLogDetNonBatch):\n",
    "    seed = 0\n",
    "    matrix_shape = torch.Size((3, 50, 50))\n",
    "\n",
    "    def test_inv_quad_logdet_vector(self):\n",
    "        pass\n",
    "\n",
    "    def test_precond_inv_quad_logdet_vector(self):\n",
    "        pass\n",
    "\n",
    "    def test_inv_quad_only_vector(self):\n",
    "        pass\n",
    "\n",
    "    def test_precond_inv_quad_only_vector(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TestInvQuadLogDetMultiBatch(TestInvQuadLogDetBatch):\n",
    "    seed = 0\n",
    "    matrix_shape = torch.Size((2, 3, 50, 50))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=[\"\"], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....................................\n",
      "----------------------------------------------------------------------\n",
      "Ran 37 tests in 4.083s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import unittest\n",
    "\n",
    "import torch\n",
    "\n",
    "import linear_operator\n",
    "from linear_operator.test.base_test_case import BaseTestCase\n",
    "from linear_operator.utils.cholesky import psd_safe_cholesky\n",
    "from linear_operator.utils.permutation import apply_permutation, inverse_permutation\n",
    "\n",
    "\n",
    "def _ensure_symmetric_grad(grad):\n",
    "    \"\"\"\n",
    "    A gradient-hook hack to ensure that symmetric matrix gradients are symmetric\n",
    "    \"\"\"\n",
    "    res = torch.add(grad, grad.mT).mul(0.5)\n",
    "    return res\n",
    "\n",
    "\n",
    "class TestPivotedCholesky(BaseTestCase, unittest.TestCase):\n",
    "    seed = 0\n",
    "\n",
    "    def _create_mat(self):\n",
    "        mat = torch.randn(8, 8)\n",
    "        mat = mat @ mat.mT\n",
    "        return mat\n",
    "\n",
    "    def test_pivoted_cholesky(self, max_iter=3):\n",
    "        mat = self._create_mat().detach().requires_grad_(True)\n",
    "        mat.register_hook(_ensure_symmetric_grad)\n",
    "        mat_copy = mat.detach().clone().requires_grad_(True)\n",
    "        mat_copy.register_hook(_ensure_symmetric_grad)\n",
    "\n",
    "        # Forward (with function)\n",
    "        res, pivots = linear_operator.pivoted_cholesky(mat, rank=max_iter, return_pivots=True)\n",
    "\n",
    "        # Forward (manual pivoting, actual Cholesky)\n",
    "        inverse_pivots = inverse_permutation(pivots)\n",
    "        # Apply pivoting\n",
    "        pivoted_mat_copy = apply_permutation(mat_copy, pivots, pivots)\n",
    "        # Compute Cholesky\n",
    "        actual_pivoted = psd_safe_cholesky(pivoted_mat_copy)[..., :max_iter]\n",
    "        # Undo pivoting\n",
    "        actual = apply_permutation(actual_pivoted, left_permutation=inverse_pivots)\n",
    "\n",
    "        self.assertAllClose(res, actual)\n",
    "\n",
    "        # Backward\n",
    "        grad_output = torch.randn_like(res)\n",
    "        res.backward(gradient=grad_output)\n",
    "        actual.backward(gradient=grad_output)\n",
    "        self.assertAllClose(mat.grad, mat_copy.grad)\n",
    "\n",
    "\n",
    "class TestPivotedCholeskyBatch(TestPivotedCholesky, unittest.TestCase):\n",
    "    seed = 0\n",
    "\n",
    "    def _create_mat(self):\n",
    "        mat = torch.randn(2, 3, 8, 8)\n",
    "        mat = mat @ mat.mT\n",
    "        return mat\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=[\"\"], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# linear_operator packages\n",
    "from linear_operator import inv_quad_logdet\n",
    "from linear_operator.operators import DenseLinearOperator\n",
    "\n",
    "class IterativeGP(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple illustration of using linear_operator's inv_quad_logdet\n",
    "    to fit a GP model given a kernel matrix + diagonal noise.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel, noise=0.1):\n",
    "        super().__init__()\n",
    "        # \"kernel\" can be something from GPyTorch (e.g. RBFKernel),\n",
    "        # or any callable that can produce an (n x n) covariance matrix\n",
    "        self.kernel = kernel\n",
    "        # We make noise a Parameter so that it can be trained via gradient\n",
    "        self.raw_noise = nn.Parameter(torch.log(torch.tensor(noise, dtype=torch.float32)))\n",
    "\n",
    "    def noise_value(self):\n",
    "        # Use softplus or exponent to keep noise strictly positive\n",
    "        return torch.exp(self.raw_noise)\n",
    "\n",
    "    def fit(self, train_x, train_y):\n",
    "        \"\"\"\n",
    "        train_x: tensor of shape (n x d)\n",
    "        train_y: tensor of shape (n,) or (n x 1)\n",
    "\n",
    "        Returns: The scalar MLL (float) after computing inv_quad and logdet.\n",
    "        \"\"\"\n",
    "        train_x = train_x.detach().clone()\n",
    "        train_y = train_y.detach().clone()\n",
    "\n",
    "        # Mark them as requiring gradient only if you want to backprop\n",
    "        # through train_x or train_y themselves (usually you do not)\n",
    "        # train_x.requires_grad_(False)\n",
    "        # train_y.requires_grad_(False)\n",
    "\n",
    "        # Produce a dense covariance matrix from the kernel\n",
    "        K = self.kernel(train_x, train_x).evaluate()  # shape (n, n)\n",
    "        # Add noise on the diagonal\n",
    "        K = K + self.noise_value() * torch.eye(K.size(-1), dtype=K.dtype, device=K.device)\n",
    "\n",
    "        # Wrap the matrix with DenseLinearOperator\n",
    "        linear_op = DenseLinearOperator(K)\n",
    "\n",
    "        # If train_y is just shape (n,), pass it as inv_quad_rhs\n",
    "        # logdet=True means we also want the approximate log|K|\n",
    "        inv_quad_term, logdet_term = inv_quad_logdet(\n",
    "            linear_op,\n",
    "            inv_quad_rhs=train_y,\n",
    "            logdet=True\n",
    "        )\n",
    "\n",
    "        n = train_y.size(0)\n",
    "        # Standard Gaussian log-likelihood:\n",
    "        # 0.5 * ( y^T K^{-1} y + log|K| + n * log(2π) )\n",
    "        const = n * torch.log(torch.tensor(2.0 * 3.141592653589793, dtype=K.dtype, device=K.device))\n",
    "        mll = 0.5 * (inv_quad_term + logdet_term + const)\n",
    "\n",
    "        # Backprop:\n",
    "        mll.backward()  # This will compute d/d(kernel params) and d/d(noise)\n",
    "        \n",
    "        # Return the numeric value\n",
    "        return mll.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "from gpytorch.kernels import RBFKernel,MaternKernel,ScaleKernel\n",
    "from gpytorch.priors import GammaPrior\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "notebook_dir = os.getcwd()\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '../code'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "from gps import CholeskyGaussianProcess\n",
    "from plotting import plot_gp_simple,plot_gp_sample\n",
    "from util import train,eval,plot_gpr_results\n",
    "from gps import IterativeGaussianProcess,CholeskyGaussianProcess\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# np.random.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "device=\"cuda:0\"\n",
    "global_dtype=torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data generation function with an abstract true function\n",
    "def generate_data(true_function, train_range=(-3, 3), test_range=(-3, 3), \n",
    "                  n_train=40, n_test=100, noise_std=0.1, \n",
    "                  device='cuda:0', dtype=torch.float64):\n",
    "    # Generate training data\n",
    "    X_train = torch.linspace(train_range[0], train_range[1], n_train, dtype=dtype, device=device).unsqueeze(-1)\n",
    "    y_train = true_function(X_train) + noise_std * torch.randn_like(X_train)\n",
    "    \n",
    "    # Generate test data\n",
    "    X_test = torch.linspace(test_range[0], test_range[1], n_test, dtype=dtype, device=device).unsqueeze(-1)\n",
    "    y_test = true_function(X_test)  # No noise added to test data\n",
    "    \n",
    "    return X_train, y_train.squeeze(), X_test, y_test.squeeze()\n",
    "\n",
    "# Define the true function\n",
    "def true_function(x):\n",
    "    \n",
    "    return torch.sin(2 * x) + torch.cos(3 * x)\n",
    "\n",
    "# Generate data using the true function\n",
    "train_x, train_y, test_x, test_y = generate_data(true_function,train_range=(-3, 3), test_range=(-5,5), dtype=global_dtype)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from linear_operator.operators import DenseLinearOperator\n",
    "from linear_operator import inv_quad_logdet\n",
    "\n",
    "\n",
    "# class IterativeGP(nn.Module):\n",
    "#     \"\"\"\n",
    "#     A minimal iterative Gaussian Process model that:\n",
    "#       - Does a single pass of fit(...) with inv_quad_logdet\n",
    "#       - Caches logdet(K) and inv_quad term for repeated usage\n",
    "#       - Provides a separate compute_mll(...) method\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, kernel, noise=0.1, dtype=torch.float64, device=\"cuda:0\"):\n",
    "#         super().__init__()\n",
    "#         self.kernel = kernel\n",
    "#         self.dtype = dtype\n",
    "#         self.device = device\n",
    "#         # Use a raw noise parameter if you want to backprop through noise\n",
    "#         self.raw_noise = nn.Parameter(torch.log(torch.tensor(noise, dtype=self.dtype, device=self.device)))\n",
    "\n",
    "#         # We'll cache these during fit\n",
    "#         self.cached_logdet = None\n",
    "#         self.cached_inv_quad = None\n",
    "#         self.cached_mll = None\n",
    "\n",
    "#     def noise_value(self):\n",
    "#         \"\"\"\n",
    "#         Exponentiate raw_noise to keep it strictly > 0\n",
    "#         \"\"\"\n",
    "#         return torch.exp(self.raw_noise)\n",
    "\n",
    "#     def fit(self, train_x, train_y):\n",
    "#         \"\"\"\n",
    "#         Perform a single 'fit' step:\n",
    "#          1) Build K = kernel(...) + noise * I\n",
    "#          2) inv_quad_logdet(...) => y^T K^{-1} y, logdet(K)\n",
    "#          3) Store/copy to self.cached_(...) variables\n",
    "#          4) (Optional) call backward if you want to do gradient-based updates right away\n",
    "#         \"\"\"\n",
    "#         train_x = train_x.to(self.device, self.dtype)\n",
    "#         train_y = train_y.to(self.device, self.dtype)\n",
    "\n",
    "#         # Build kernel matrix\n",
    "#         K = self.kernel(train_x, train_x).evaluate()  # (n x n)\n",
    "#         K = K + self.noise_value() * torch.eye(\n",
    "#             K.size(0), dtype=self.dtype, device=self.device\n",
    "#         )\n",
    "\n",
    "#         # Wrap in DenseLinearOperator\n",
    "#         lin_op = DenseLinearOperator(K)\n",
    "\n",
    "#         # Compute inverse-quad & logdet\n",
    "#         # inv_quad_rhs=train_y => y^T K^{-1} y\n",
    "#         # logdet=True => approximate log|K|\n",
    "#         inv_quad_term, logdet_term = inv_quad_logdet(\n",
    "#             lin_op, inv_quad_rhs=train_y, logdet=True\n",
    "#         )\n",
    "\n",
    "#         # Optionally store for repeated usage\n",
    "#         self.cached_inv_quad = inv_quad_term.detach()\n",
    "#         self.cached_logdet = logdet_term.detach()\n",
    "\n",
    "#         # Also store the final MLL if you like\n",
    "#         n = train_y.size(0)\n",
    "#         const = n * torch.log(torch.tensor(2.0 * 3.141592653589793, dtype=self.dtype, device=self.device))\n",
    "#         mll = 0.5 * (inv_quad_term + logdet_term + const)\n",
    "\n",
    "#         self.cached_mll = mll.detach()\n",
    "\n",
    "#         # If you want to do a single backward pass right now, uncomment:\n",
    "#         # mll.backward()\n",
    "\n",
    "#     def compute_mll(self):\n",
    "#         \"\"\"\n",
    "#         Given that fit(...) has cached inv_quad and logdet,\n",
    "#         compute the same MLL without re-running inv_quad_logdet.\n",
    "#         \"\"\"\n",
    "#         if self.cached_inv_quad is None or self.cached_logdet is None:\n",
    "#             raise RuntimeError(\"Must call fit(...) first to cache inv_quad/logdet values.\")\n",
    "\n",
    "#         # We'll need to know the size of training data for the constant term\n",
    "#         # but we haven't stored train_y. Let's assume we also stored n in fit:\n",
    "#         n = self.cached_inv_quad.size(0) if self.cached_inv_quad.dim() > 0 else 1\n",
    "#         # Actually, if y was shape (n,), then inv_quad is just a scalar,\n",
    "#         # so we can't parse n from that. Instead, suppose we stored `self.n_train` in fit(...).\n",
    "\n",
    "#         # If we have self.cached_mll from fit, we can just return that:\n",
    "#         if self.cached_mll is not None:\n",
    "#             return self.cached_mll\n",
    "\n",
    "#         # Or recompute from cached scalars:\n",
    "#         #  mll = 0.5 * ( self.cached_inv_quad + self.cached_logdet + n*log(2π) )\n",
    "#         #  return mll\n",
    "\n",
    "#         raise NotImplementedError(\"Either store self.n_train or store self.cached_mll in fit(...) and just return it.\")\n",
    "\n",
    "#     def predict(self, X_star):\n",
    "#         \"\"\"\n",
    "#         Optional. If you want to do posterior prediction with CG solves:\n",
    "#          - Build (K + noise*I) again\n",
    "#          - Solve for alpha = K^{-1} y\n",
    "#          - Then compute K(X_star, X) alpha for the predictive mean\n",
    "#          - Possibly multiple solves for the predictive covariance\n",
    "#         We'll keep it minimal and leave it as a placeholder.\n",
    "#         \"\"\"\n",
    "#         raise NotImplementedError(\"Implement iterative solves for predictive mean/cov if desired.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "from linear_operator import settings\n",
    "from linear_operator.utils.lanczos import lanczos_tridiag_to_diag\n",
    "from linear_operator.utils.stochastic_lq import StochasticLQ\n",
    "\n",
    "\n",
    "class customInvQuadLogdet(Function):\n",
    "    \"\"\"\n",
    "    Given a PSD matrix A (or a batch of PSD matrices A), this function computes one or both\n",
    "    of the following\n",
    "    - The matrix solves A^{-1} b\n",
    "    - logdet(A)\n",
    "\n",
    "    This function uses preconditioned CG and Lanczos quadrature to compute the inverse quadratic\n",
    "    and log determinant terms, using the variance reduction strategy outlined in:\n",
    "    ``Reducing the Variance of Gaussian Process Hyperparameter Optimization with Preconditioning''\n",
    "    (https://arxiv.org/abs/2107.00243)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx,\n",
    "        representation_tree,\n",
    "        precond_representation_tree,\n",
    "        preconditioner,\n",
    "        num_precond_args,\n",
    "        inv_quad,\n",
    "        probe_vectors,\n",
    "        probe_vector_norms,\n",
    "        *args,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        *args - The arguments representing the PSD matrix A (or batch of PSD matrices A)\n",
    "        If self.inv_quad is true, the first entry in *args is inv_quad_rhs (Tensor)\n",
    "        - the RHS of the matrix solves.\n",
    "\n",
    "        Returns:\n",
    "        - (Scalar) The inverse quadratic form (or None, if self.inv_quad is False)\n",
    "        - (Scalar) The log determinant (or None, self.if logdet is False)\n",
    "        \"\"\"\n",
    "\n",
    "        ctx.representation_tree = representation_tree\n",
    "        ctx.precond_representation_tree = precond_representation_tree\n",
    "        ctx.preconditioner = preconditioner\n",
    "        ctx.inv_quad = inv_quad\n",
    "        ctx.num_precond_args = num_precond_args\n",
    "\n",
    "        matrix_args = None\n",
    "        precond_args = tuple()\n",
    "        inv_quad_rhs = None\n",
    "        if ctx.inv_quad:\n",
    "            inv_quad_rhs = args[0]\n",
    "            args = args[1:]\n",
    "        if ctx.num_precond_args:\n",
    "            matrix_args = args[:-num_precond_args]\n",
    "            precond_args = args[-num_precond_args:]\n",
    "        else:\n",
    "            matrix_args = args\n",
    "\n",
    "        # Get closure for matmul\n",
    "        linear_op = ctx.representation_tree(*matrix_args)\n",
    "        precond_lt = ctx.precond_representation_tree(*precond_args)\n",
    "\n",
    "        # Get info about matrix\n",
    "        ctx.dtype = linear_op.dtype\n",
    "        ctx.device = linear_op.device\n",
    "        ctx.matrix_shape = linear_op.matrix_shape\n",
    "        ctx.batch_shape = linear_op.batch_shape\n",
    "\n",
    "        # Probe vectors\n",
    "        if probe_vectors is None or probe_vector_norms is None:\n",
    "            num_random_probes = settings.num_trace_samples.value()\n",
    "            if settings.deterministic_probes.on():\n",
    "                # NOTE: calling precond_lt.root_decomposition() is expensive\n",
    "                # because it requires Lanczos\n",
    "                # We don't have any other choice for when we want to use deterministic probes, however\n",
    "                if precond_lt.size()[-2:] == torch.Size([1, 1]):\n",
    "                    covar_root = precond_lt.to_dense().sqrt()\n",
    "                else:\n",
    "                    covar_root = precond_lt.root_decomposition().root\n",
    "\n",
    "                warnings.warn(\n",
    "                    \"The deterministic probes feature is now deprecated. \"\n",
    "                    \"See https://github.com/cornellius-gp/linear_operator/pull/1836.\",\n",
    "                    DeprecationWarning,\n",
    "                )\n",
    "                base_samples = settings.deterministic_probes.probe_vectors\n",
    "                if base_samples is None or covar_root.size(-1) != base_samples.size(-2):\n",
    "                    base_samples = torch.randn(\n",
    "                        *precond_lt.batch_shape,\n",
    "                        covar_root.size(-1),\n",
    "                        num_random_probes,\n",
    "                        dtype=precond_lt.dtype,\n",
    "                        device=precond_lt.device,\n",
    "                    )\n",
    "                    settings.deterministic_probes.probe_vectors = base_samples\n",
    "\n",
    "                probe_vectors = covar_root.matmul(base_samples).permute(-1, *range(precond_lt.dim() - 1))\n",
    "            else:\n",
    "                probe_vectors = precond_lt.zero_mean_mvn_samples(num_random_probes)\n",
    "            probe_vectors = probe_vectors.unsqueeze(-2).transpose(0, -2).squeeze(0).mT.contiguous()\n",
    "            probe_vector_norms = torch.norm(probe_vectors, p=2, dim=-2, keepdim=True)\n",
    "            probe_vectors = probe_vectors.div(probe_vector_norms)\n",
    "\n",
    "        # Probe vectors\n",
    "        ctx.probe_vectors = probe_vectors\n",
    "        ctx.probe_vector_norms = probe_vector_norms\n",
    "\n",
    "        # Collect terms for LinearCG\n",
    "        # We use LinearCG for both matrix solves and for stochastically estimating the log det\n",
    "        rhs_list = [ctx.probe_vectors]\n",
    "        num_random_probes = ctx.probe_vectors.size(-1)\n",
    "        num_inv_quad_solves = 0\n",
    "\n",
    "        # RHS for inv_quad\n",
    "        ctx.is_vector = False\n",
    "        if ctx.inv_quad:\n",
    "            if inv_quad_rhs.ndimension() == 1:\n",
    "                inv_quad_rhs = inv_quad_rhs.unsqueeze(-1)\n",
    "                ctx.is_vector = True\n",
    "            rhs_list.append(inv_quad_rhs)\n",
    "            num_inv_quad_solves = inv_quad_rhs.size(-1)\n",
    "\n",
    "        # Perform solves (for inv_quad) and tridiagonalization (for estimating logdet)\n",
    "        rhs = torch.cat(rhs_list, -1)\n",
    "        solves, t_mat = linear_op._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
    "\n",
    "        # Final values to return\n",
    "        logdet_term = torch.zeros(linear_op.batch_shape, dtype=ctx.dtype, device=ctx.device)\n",
    "        inv_quad_term = torch.zeros(linear_op.batch_shape, dtype=ctx.dtype, device=ctx.device)\n",
    "\n",
    "        # Compute logdet from tridiagonalization\n",
    "        if settings.skip_logdet_forward.off():\n",
    "            if torch.any(torch.isnan(t_mat)).item():\n",
    "                logdet_term = torch.tensor(float(\"nan\"), dtype=ctx.dtype, device=ctx.device)\n",
    "            else:\n",
    "                if ctx.batch_shape is None:\n",
    "                    t_mat = t_mat.unsqueeze(1)\n",
    "                eigenvalues, eigenvectors = lanczos_tridiag_to_diag(t_mat)\n",
    "                slq = StochasticLQ()\n",
    "                (logdet_term,) = slq.to_dense(ctx.matrix_shape, eigenvalues, eigenvectors, [lambda x: x.log()])\n",
    "\n",
    "        # Extract inv_quad solves from all the solves\n",
    "        if ctx.inv_quad:\n",
    "            inv_quad_solves = solves.narrow(-1, num_random_probes, num_inv_quad_solves)\n",
    "            inv_quad_term = (inv_quad_solves * inv_quad_rhs).sum(-2)\n",
    "\n",
    "        ctx.num_random_probes = num_random_probes\n",
    "        ctx.num_inv_quad_solves = num_inv_quad_solves\n",
    "\n",
    "        to_save = list(precond_args) + list(matrix_args) + [solves]\n",
    "        ctx.save_for_backward(*to_save)\n",
    "\n",
    "        return inv_quad_term, logdet_term\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, inv_quad_grad_output, logdet_grad_output):\n",
    "        # Get input arguments, and get gradients in the proper form\n",
    "        if ctx.num_precond_args:\n",
    "            precond_args = ctx.saved_tensors[: ctx.num_precond_args]\n",
    "            matrix_args = ctx.saved_tensors[ctx.num_precond_args : -1]\n",
    "        else:\n",
    "            precond_args = []\n",
    "            matrix_args = ctx.saved_tensors[:-1]\n",
    "        solves = ctx.saved_tensors[-1]\n",
    "\n",
    "        linear_op = ctx.representation_tree(*matrix_args)\n",
    "        precond_lt = ctx.precond_representation_tree(*precond_args)\n",
    "\n",
    "        # Fix grad_output sizes\n",
    "        if ctx.inv_quad:\n",
    "            inv_quad_grad_output = inv_quad_grad_output.unsqueeze(-2)\n",
    "        logdet_grad_output = logdet_grad_output.unsqueeze(-1)\n",
    "        logdet_grad_output.unsqueeze_(-1)\n",
    "\n",
    "        # Un-normalize probe vector solves\n",
    "        coef = 1.0 / ctx.probe_vectors.size(-1)\n",
    "        probe_vector_solves = solves.narrow(-1, 0, ctx.num_random_probes).mul(coef)\n",
    "        probe_vector_solves.mul_(ctx.probe_vector_norms).mul_(logdet_grad_output)\n",
    "\n",
    "        # Apply preconditioner to probe vectors (originally drawn from N(0, P))\n",
    "        # Now the probe vectors will be drawn from N(0, P^{-1})\n",
    "        if ctx.preconditioner is not None:\n",
    "            precond_probe_vectors = ctx.preconditioner(ctx.probe_vectors * ctx.probe_vector_norms)\n",
    "        else:\n",
    "            precond_probe_vectors = ctx.probe_vectors * ctx.probe_vector_norms\n",
    "\n",
    "        # matrix gradient\n",
    "        # Collect terms for arg grads\n",
    "        left_factors_list = [probe_vector_solves]\n",
    "        right_factors_list = [precond_probe_vectors]\n",
    "\n",
    "        inv_quad_solves = None\n",
    "        neg_inv_quad_solves_times_grad_out = None\n",
    "        if ctx.inv_quad:\n",
    "            inv_quad_solves = solves.narrow(-1, ctx.num_random_probes, ctx.num_inv_quad_solves)\n",
    "            neg_inv_quad_solves_times_grad_out = inv_quad_solves.mul(inv_quad_grad_output).mul_(-1)\n",
    "            left_factors_list.append(neg_inv_quad_solves_times_grad_out)\n",
    "            right_factors_list.append(inv_quad_solves)\n",
    "\n",
    "        left_factors = torch.cat(left_factors_list, -1)\n",
    "        right_factors = torch.cat(right_factors_list, -1)\n",
    "        matrix_arg_grads = linear_op._bilinear_derivative(left_factors, right_factors)\n",
    "\n",
    "        # precond gradient\n",
    "        precond_arg_grads = precond_lt._bilinear_derivative(\n",
    "            -precond_probe_vectors * coef, precond_probe_vectors * logdet_grad_output\n",
    "        )\n",
    "\n",
    "        # inv_quad_rhs gradients\n",
    "        if ctx.inv_quad:\n",
    "            inv_quad_rhs_grad = neg_inv_quad_solves_times_grad_out.mul_(-2)\n",
    "            if ctx.is_vector:\n",
    "                inv_quad_rhs_grad.squeeze_(-1)\n",
    "            res = [inv_quad_rhs_grad] + list(matrix_arg_grads) + list(precond_arg_grads)\n",
    "        else:\n",
    "            res = list(matrix_arg_grads) + list(precond_arg_grads)\n",
    "\n",
    "        return tuple([None] * 7 + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(76.8859, device='cuda:0', dtype=torch.float64, grad_fn=<SumBackward1>)\n",
      "tensor(-119.1794, device='cuda:0', dtype=torch.float64, grad_fn=<SumBackward1>)\n",
      "15.610799816887443\n",
      "Gpytorch mll tensor(inf, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "cholgp 27.37256775093035\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.priors import GammaPrior\n",
    "from linear_operator import inv_quad_logdet\n",
    "from gpytorch.lazy.lazy_tensor import LazyTensor\n",
    "\n",
    "\n",
    "def custom_inv_quad_logdet(lazy_tsr, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):\n",
    "    assert isinstance(lazy_tsr, LazyTensor)\n",
    "\n",
    "    # Special case: use Cholesky to compute these terms\n",
    "    if settings.fast_computations.log_prob.off() or (lazy_tsr.size(-1) <= settings.max_cholesky_size.value()):\n",
    "        from linear_operator.operators.chol_linear_operator import CholLinearOperator\n",
    "        from linear_operator.operators.chol_linear_operator import TriangularLinearOperator\n",
    "\n",
    "        cholesky = CholLinearOperator(TriangularLinearOperator(lazy_tsr.cholesky()))\n",
    "        return cholesky.inv_quad_logdet(inv_quad_rhs=inv_quad_rhs, logdet=logdet, reduce_inv_quad=reduce_inv_quad)\n",
    "\n",
    "    # Default: use modified batch conjugate gradients to compute these terms\n",
    "    # See NeurIPS 2018 paper: https://arxiv.org/abs/1809.11165\n",
    "    if not lazy_tsr.is_square:\n",
    "        raise RuntimeError(\n",
    "            \"inv_quad_logdet only operates on (batches of) square (positive semi-definite) LazyTensors. \"\n",
    "            \"Got a {} of size {}.\".format(lazy_tsr.__class__.__name__, lazy_tsr.size())\n",
    "        )\n",
    "\n",
    "    if inv_quad_rhs is not None:\n",
    "        if lazy_tsr.dim() == 2 and inv_quad_rhs.dim() == 1:\n",
    "            if lazy_tsr.shape[-1] != inv_quad_rhs.numel():\n",
    "                raise RuntimeError(\n",
    "                    \"LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).\".format(\n",
    "                        lazy_tsr.shape, inv_quad_rhs.shape\n",
    "                    )\n",
    "                )\n",
    "        elif lazy_tsr.dim() != inv_quad_rhs.dim():\n",
    "            raise RuntimeError(\n",
    "                \"LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number \"\n",
    "                \"of dimensions.\".format(lazy_tsr.shape, inv_quad_rhs.shape)\n",
    "            )\n",
    "        elif lazy_tsr.batch_shape != inv_quad_rhs.shape[:-2] or lazy_tsr.shape[-1] != inv_quad_rhs.shape[-2]:\n",
    "            raise RuntimeError(\n",
    "                \"LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={}).\".format(\n",
    "                    lazy_tsr.shape, inv_quad_rhs.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "    args = lazy_tsr.representation()  # TODO: check this\n",
    "\n",
    "    if inv_quad_rhs is not None:\n",
    "        args = [inv_quad_rhs] + list(args)\n",
    "\n",
    "    probe_vectors, probe_vector_norms = lazy_tsr._probe_vectors_and_norms()\n",
    "\n",
    "    func = CustomInvQuadLogDet.apply\n",
    "\n",
    "    inv_quad_term, logdet_term = func(\n",
    "        lazy_tsr.representation_tree(),\n",
    "        lazy_tsr.dtype,\n",
    "        lazy_tsr.device,\n",
    "        lazy_tsr.matrix_shape,\n",
    "        lazy_tsr.batch_shape,\n",
    "        (inv_quad_rhs is not None),\n",
    "        logdet,\n",
    "        probe_vectors,\n",
    "        probe_vector_norms,\n",
    "        *args,\n",
    "    )\n",
    "\n",
    "    if inv_quad_term.numel() and reduce_inv_quad:\n",
    "        inv_quad_term = inv_quad_term.sum(-1)\n",
    "    return inv_quad_term, logdet_term\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(f, train_range=(-3, 3), test_range=(-3, 3), n_train=40, n_test=100, noise_std=0.1, device='cuda:0', dtype=torch.float64):\n",
    "    X_train = torch.linspace(train_range[0], train_range[1], n_train, dtype=dtype, device=device).unsqueeze(-1)\n",
    "    y_train = f(X_train) + noise_std * torch.randn_like(X_train)\n",
    "    X_test = torch.linspace(test_range[0], test_range[1], n_test, dtype=dtype, device=device).unsqueeze(-1)\n",
    "    y_test = f(X_test)\n",
    "    return X_train, y_train.squeeze(), X_test, y_test.squeeze()\n",
    "\n",
    "def true_function(x):\n",
    "    return torch.sin(2 * x) + torch.cos(3 * x)\n",
    "\n",
    "class IterativeGP2(nn.Module):\n",
    "    def __init__(self, kernel, noise=0.1, dtype=torch.float64, device=\"cuda:0\"):\n",
    "        super().__init__()\n",
    "        self.kernel = kernel\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.raw_noise = nn.Parameter(torch.log(torch.tensor(noise, dtype=self.dtype, device=self.device)))\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.logdet_term = None\n",
    "\n",
    "    def noise_value(self):\n",
    "        return torch.exp(self.raw_noise)\n",
    "\n",
    "    def fit(self, train_x, train_y):\n",
    "        self.train_x = train_x.to(self.device, self.dtype)\n",
    "        self.train_y = train_y.to(self.device, self.dtype)\n",
    "\n",
    "        self.K = self.kernel(self.train_x, self.train_x)\n",
    "        self.K.add_diag(self.noise_value()**2)\n",
    "        inv_quad,logdet = inv_quad_logdet(self.K, inv_quad_rhs=train_y.unsqueeze(1), logdet=True, reduce_inv_quad=True)\n",
    "        print(inv_quad)\n",
    "        print(logdet)\n",
    "        self.logdet_term= logdet\n",
    "        self.inv_quad_term = inv_quad\n",
    "\n",
    "    def compute_mll(self):\n",
    "        n = self.train_y.size(0)\n",
    "        const = n * torch.log(torch.tensor(2.0 * 3.141592653589793, dtype=self.dtype, device=self.device))\n",
    "        return 0.5 * (self.inv_quad_term + self.logdet_term + const)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_x, train_y, test_x, test_y = generate_data(true_function, train_range=(-3,3), test_range=(-5,5), device=device)\n",
    "base_kernel = MaternKernel(ard_num_dims=train_x.shape[-1], nu=1.5, lengthscale_prior=GammaPrior(-3.0, 3.0))\n",
    "kernel = ScaleKernel(base_kernel, outputscale_prior=GammaPrior(-3.0, 3.0)).to(device)\n",
    "igp1 = IterativeGP2(kernel, noise=0.4, dtype=torch.float64, device=device)\n",
    "igp1.fit(train_x, train_y)\n",
    "mll_value = igp1.compute_mll()\n",
    "print(mll_value.item())\n",
    "\n",
    "import gpytorch\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        base_kernel = MaternKernel(ard_num_dims=train_x.shape[-1], nu=1.5, lengthscale_prior=GammaPrior(-3.0, 3.0))\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = ScaleKernel(base_kernel, outputscale_prior=GammaPrior(-3.0, 3.0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_x, train_y, test_x, test_y = generate_data(true_function, train_range=(-3, 3), test_range=(-5, 5), device=device)\n",
    "\n",
    "# Initialize likelihood and the exact GP model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "model = ExactGPModel(train_x, train_y, likelihood).to(device)\n",
    "\n",
    "# Train the model by maximizing the marginal log likelihood\n",
    "model.train()\n",
    "likelihood.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "output = model(train_x)\n",
    "loss = -mll(output, train_y)\n",
    "print(\"Gpytorch mll\",loss)\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_x, train_y, test_x, test_y = generate_data(true_function, train_range=(-3,3), test_range=(-5,5), device=device)\n",
    "base_kernel = MaternKernel(ard_num_dims=train_x.shape[-1], nu=1.5, lengthscale_prior=GammaPrior(-3.0, 3.0))\n",
    "kernel = ScaleKernel(base_kernel, outputscale_prior=GammaPrior(-3.0, 3.0)).to(device)\n",
    "igp2 = CholeskyGaussianProcess(kernel, noise=0.4, dtype=torch.float64, device=device)\n",
    "igp2.fit(train_x, train_y)\n",
    "mll_value = igp2.compute_mll(train_y)\n",
    "print(\"cholgp\",mll_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
